{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7dca096",
      "metadata": {
        "id": "f7dca096"
      },
      "source": [
        "# CI7520 â€“Assignment 1: Classical Machine Learning - Part II - Clustering\n",
        "\n",
        "**Submitted by:**\n",
        "\n",
        "1) Adrian Bandy (K2132274)\n",
        "\n",
        "2) Shashwat Bhardwaj (K2149137)\n",
        "\n",
        "3) Royce Daran Shakespeare (K2046699)\n",
        "\n",
        "4) Padmesh Upadhyay (K2136572)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction\n",
        "\n",
        "In this assignment we will be exploring a variety of clustering and classification models using sklearn and the breast cancer dataset."
      ],
      "metadata": {
        "id": "LE_rIhJ2gMr2"
      },
      "id": "LE_rIhJ2gMr2"
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "id": "b8e7f2ef",
      "metadata": {
        "scrolled": true,
        "id": "b8e7f2ef"
      },
      "outputs": [],
      "source": [
        "#Importing The Required Libraries\n",
        "\n",
        "#Data and data manipulation libraries\n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.pylabtools import figsize\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.inspection import plot_partial_dependence\n",
        "\n",
        "\n",
        "#Data preprocessing\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#Models\n",
        "from sklearn import cluster\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "#Hyperparameter Tuning and processing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "#Scoring and Performance Evaluation\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import calinski_harabasz_score\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "from sklearn.metrics.cluster import homogeneity_completeness_v_measure\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import (precision_recall_curve, PrecisionRecallDisplay)\n",
        "from sklearn.metrics import homogeneity_score\n",
        "from sklearn.metrics import completeness_score\n",
        "from sklearn.metrics import v_measure_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score, make_scorer\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declared Functions\n",
        "\n",
        "Using the DRY (Dont Repeat Yourself) method as outlined in The Pragmatic Programmer [1] we will declare functions that we use multiple times.\n",
        "This has the benefit of less and cleaner code, which is far easier to update or maintain.\n",
        "\n",
        "##### [1] Hunt, A., Thomas, D. and Cunningham, W., 2015. The pragmatic programmer. Boston: Addison-Wesley, Chapter 2."
      ],
      "metadata": {
        "id": "8poPyxOQgqS4"
      },
      "id": "8poPyxOQgqS4"
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "id": "e3f8b665",
      "metadata": {
        "id": "e3f8b665"
      },
      "outputs": [],
      "source": [
        "def clustering_performance_evaluation(X, predicted_labels, print_output = False):\n",
        "  '''\n",
        "  Accepts data of shape 2d and the predicted labels\n",
        "  Returns a series of different clustering scores\n",
        "  If print_output = True it will print the scores as well as returning them\n",
        "  '''\n",
        "  #silhouette is mean intra-cluster distance - nearest-cluster distance\n",
        "  #divided by the maximum of these two.\n",
        "  #Values between 1 and -1. 1 being best score achievable.\n",
        "  sil_score = silhouette_score(X, predicted_labels)\n",
        "        \n",
        "  #Calinski-Harabasz score\n",
        "  #Ratio of sum of the inter-cluster dispersion and intra-cluster dispersion\n",
        "  #Higher values better\n",
        "  cal_har_score = calinski_harabasz_score(X, predicted_labels)\n",
        "    \n",
        "  #David-Bouldin Score\n",
        "  #Avg similarity within cluster. \n",
        "  #Similarity is ratio of intra-cluster to inter-cluster\n",
        "  #Low values are better (minima is 0)\n",
        "  db_score = davies_bouldin_score(X, predicted_labels)\n",
        "      \n",
        "  if print_output == True:\n",
        "      print(f\"Silhouette Score: {sil_score:.3f}\\\n",
        "      \\nCalinski-Harabasz Score: {cal_har_score:.3f}\\\n",
        "      \\nDavid-Bouldin Score: {db_score:.3f}\")\n",
        "    \n",
        "  if print_output == False:\n",
        "      return [sil_score, cal_har_score, db_score]\n",
        "\n",
        "def classification_performance_evaluation(y_true, y_pred, print_output = False):\n",
        "  '''\n",
        "  Accepts data of the ground truth labels and the predicted labels\n",
        "  Returns a series of different classification scores\n",
        "  If print_output = True it will print the scores as well as returning them\n",
        "  '''\n",
        "  #Both homogeneity and completeness have values 0 to 1. \n",
        "  #Larger values being preferable\n",
        "  #V measure is a 'harmonic' mean of homogeneity and completeness\n",
        "  h,c,v = homogeneity_completeness_v_measure(y_true, y_pred)\n",
        "    \n",
        "  #accuracy score displays % of labels correctly labelled.\n",
        "  acc_score = accuracy_score(y_true, y_pred)\n",
        "    \n",
        "  #balanced accuracy to adjust for inbalanced classes\n",
        "  bal_acc_score = balanced_accuracy_score(y_true, y_pred)\n",
        "    \n",
        "  #f1_score\n",
        "  #Harmonic mean of precision and recall\n",
        "  #Values of 0 to 1. 1 is best.\n",
        "  score_f1 = f1_score(y_true, y_pred)\n",
        "    \n",
        "  if print_output == True:\n",
        "      print(f\"Homogeneity Score: {h:.3f}\\\n",
        "      \\nCompleteness Score: {c:.3f}\\\n",
        "      \\nV_measure Score: {v:.3f}\\\n",
        "      \\nAccuracy Score: {acc_score:.3f}\\\n",
        "      \\nBalanced Accuracy Score: {bal_acc_score:.3f}\\\n",
        "      \\nF1 Score: {score_f1:.3f}\")\n",
        "\n",
        "  if print_output == False:\n",
        "      return [h, c, v, acc_score, bal_acc_score, score_f1]\n",
        "\n",
        "def classification_using_clustering_performance_evaluation(y_true, y_pred, print_output = False, classification = True):\n",
        "  '''\n",
        "  Accepts data of the ground truth labels and the predicted labels\n",
        "  Returns a series of different clustering and classification scores\n",
        "  If print_output = True it will print the scores as well as returning them\n",
        "  If classification = False will correct accuracy metrics (because clustering does not ascribe which cluster is which label)\n",
        "  '''\n",
        "  #Both homogeneity and completeness have values 0 to 1. \n",
        "  #Larger values being preferable\n",
        "  #V measure is a 'harmonic' mean of homogeneity and completeness\n",
        "  h,c,v = homogeneity_completeness_v_measure(y_true, y_pred)\n",
        "    \n",
        "  #accuracy score displays % of labels correctly labelled.\n",
        "  acc_score = accuracy_score(y_true, y_pred)\n",
        "    \n",
        "  #balanced accuracy to adjust for inbalanced classes\n",
        "  bal_acc_score = balanced_accuracy_score(y_true, y_pred)\n",
        "    \n",
        "  #f1_score\n",
        "  #Harmonic mean of precision and recall\n",
        "  #Values of 0 to 1. 1 is best.\n",
        "  score_f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "  #Adjusted rand score\n",
        "  #Similarity between clusters by counting all pairs of clusters,\n",
        "  #Higher values when greater proportion is in correct cluster\n",
        "  adj_rand_score = adjusted_rand_score(y_true, y_pred)\n",
        "    \n",
        "  #if a clustering method selected that did not have information about labels select classification as False\n",
        "  if classification == False:\n",
        "      #Must take maximum of 1-score and score because clustering method is only labelling two groups without\n",
        "      #assigning which is which\n",
        "      acc_score = max(acc_score, (1-acc_score))\n",
        "      bal_acc_score = max(bal_acc_score, (1-bal_acc_score))\n",
        "      score_f1 = max(score_f1, (1-score_f1))\n",
        "\n",
        "      if print_output == True:\n",
        "        print(f\"Homogeneity Score: {h:.3f}\\\n",
        "        \\nCompleteness Score: {c:.3f}\\\n",
        "        \\nV_measure Score: {v:.3f}\\\n",
        "        \\nAccuracy Score: {acc_score:.3f}\\\n",
        "        \\nBalanced Accuracy Score: {bal_acc_score:.3f}\\\n",
        "        \\nF1 Score: {score_f1:.3f}\\\n",
        "        \\nAdjusted_rand_score : {adj_rand_score:.3f}\")\n",
        "      return [h, c, v, acc_score, bal_acc_score, score_f1, adj_rand_score]\n",
        "  \n",
        "  \n",
        "  elif classification == True: \n",
        "    if print_output == True:\n",
        "      print(f\"Homogeneity Score: {h:.3f}\\\n",
        "      \\nCompleteness Score: {c:.3f}\\\n",
        "      \\nV_measure Score: {v:.3f}\\\n",
        "      \\nAccuracy Score: {acc_score:.3f}\\\n",
        "      \\nBalanced Accuracy Score: {bal_acc_score:.3f}\\\n",
        "      \\nF1 Score: {score_f1:.3f}\\\n",
        "      \\nAdjusted_rand_score : {adj_rand_score:.3f}\")\n",
        "    \n",
        "  if print_output == False:\n",
        "      return [h, c, v, acc_score, bal_acc_score, score_f1, adj_rand_score]\n",
        "    \n",
        "def plot_precision_recall(y_true, y_pred):\n",
        "  '''\n",
        "  Accepts ground truth and prediction labels\n",
        "  Plots a precision recall graph\n",
        "  '''\n",
        "  #Plots the precision_recall_graph\n",
        "  PrecisionRecallDisplay.from_predictions(y_true, y_pred)\n",
        "  plt.ylim(0,1)\n",
        "  plt.show()\n",
        "\n",
        "def plot_roc_and_print_auc(y_true, y_pred):\n",
        "  '''\n",
        "  Accepts ground truth and prediction labels\n",
        "  Plots a ROC curve with AUC calculated value\n",
        "  '''\n",
        "  #Plots the Receiver Operating Characteristic\n",
        "  #And prints the ROC_AUC (area under curve) value\n",
        "  #Values of 0 to 1. 1 is best\n",
        "  false_positive_rate, true_positive_rate, threshold  =  roc_curve(y_true, y_pred)\n",
        "  area_under_curve = round(roc_auc_score(y_true, y_pred), 3)\n",
        "  zero_to_one_array = np.linspace(0,1,100)\n",
        "    \n",
        "  #plot x = y graph\n",
        "  plt.plot(zero_to_one_array, zero_to_one_array, linestyle = 'dashed', c = 'k')\n",
        "  plt.plot(false_positive_rate, true_positive_rate, label = (f\"Area Under Curve:\\n{area_under_curve}\"))\n",
        "  plt.legend(loc = 'lower right')\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('ROC Curve and AUC Score')\n",
        "  plt.show()\n",
        "    \n",
        "def plot_confusion_matrix(y_true, y_pred):    \n",
        "  ConfusionMatrixDisplay.from_predictions(y_true, y_pred)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading The Data"
      ],
      "metadata": {
        "id": "AS5HvF4bpSC0"
      },
      "id": "AS5HvF4bpSC0"
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "id": "77a6adb5",
      "metadata": {
        "id": "77a6adb5"
      },
      "outputs": [],
      "source": [
        "#Load data as json\n",
        "bcd = datasets.load_breast_cancer()\n",
        "#Load data as frames\n",
        "# X, y = datasets.load_breast_cancer(return_X_y = True, as_frame = True)\n",
        "#Convert frames to single dataframe\n",
        "# df = pd.concat([X, y], axis =1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = bcd.data"
      ],
      "metadata": {
        "id": "LsnlfAVasnYf"
      },
      "id": "LsnlfAVasnYf",
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = bcd.target"
      ],
      "metadata": {
        "id": "FtJgdeK-GE7z"
      },
      "id": "FtJgdeK-GE7z",
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_columns = bcd.feature_names"
      ],
      "metadata": {
        "id": "8bA9II_otCgu"
      },
      "id": "8bA9II_otCgu",
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "id": "cb4ce613",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb4ce613",
        "outputId": "43d8c0d0-ab9a-4d74-d1ec-f7f6efa40201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
            " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
            " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
            " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
            " 'smoothness error' 'compactness error' 'concavity error'\n",
            " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
            " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
            " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
            " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
            "[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
            " 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
            " 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
            " 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
            " 4.601e-01 1.189e-01]\n",
            "[2.057e+01 1.777e+01 1.329e+02 1.326e+03 8.474e-02 7.864e-02 8.690e-02\n",
            " 7.017e-02 1.812e-01 5.667e-02 5.435e-01 7.339e-01 3.398e+00 7.408e+01\n",
            " 5.225e-03 1.308e-02 1.860e-02 1.340e-02 1.389e-02 3.532e-03 2.499e+01\n",
            " 2.341e+01 1.588e+02 1.956e+03 1.238e-01 1.866e-01 2.416e-01 1.860e-01\n",
            " 2.750e-01 8.902e-02]\n",
            "[1.969e+01 2.125e+01 1.300e+02 1.203e+03 1.096e-01 1.599e-01 1.974e-01\n",
            " 1.279e-01 2.069e-01 5.999e-02 7.456e-01 7.869e-01 4.585e+00 9.403e+01\n",
            " 6.150e-03 4.006e-02 3.832e-02 2.058e-02 2.250e-02 4.571e-03 2.357e+01\n",
            " 2.553e+01 1.525e+02 1.709e+03 1.444e-01 4.245e-01 4.504e-01 2.430e-01\n",
            " 3.613e-01 8.758e-02]\n"
          ]
        }
      ],
      "source": [
        "#Print first 3 rows\n",
        "print(X_columns)\n",
        "for i in range(3):\n",
        "  print(X[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2313de45",
      "metadata": {
        "id": "2313de45"
      },
      "source": [
        "## Feature Engineering\n",
        "Many of these features relate to the same aspect of the data.\n",
        "\n",
        "Therefore there is extra information that can be extracted.\n",
        "\n",
        "To do this we will create retain the feaures so far and create a variety of new derived features\n",
        "\n",
        "Coding inspired and expanded from [2]\n",
        "\n",
        "[2] Python Feature Engineering Cookbook - Packt 2020, Chapter 9 - Applying Mathematical Computations to Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "679466fd",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "679466fd",
        "outputId": "a2c00df0-e01b-4045-8312-7ccc19440a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "worst - mean radius\n",
            "worst - mean texture\n",
            "worst - mean perimeter\n",
            "worst - mean area\n",
            "worst - mean smoothness\n",
            "worst - mean compactness\n",
            "worst - mean concavity\n",
            "worst - mean concave points\n",
            "worst - mean symmetry\n",
            "worst - mean fractal dimension\n"
          ]
        }
      ],
      "source": [
        "#create new 'difference' columns\n",
        "original_col_length = len(X_columns)\n",
        "for i in range(original_col_length - 20):\n",
        "    new_col_name = str(''.join([X_columns[i+20][:5], ' - ',X_columns[i]]))\n",
        "    print(new_col_name)\n",
        "    X_columns = np.append(X_columns,new_col_name) \n",
        "    new_column_data = X[:,[i+20]] - X[:,[i]]\n",
        "    X = np.concatenate((X, new_column_data), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "id": "187b0a53",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "187b0a53",
        "outputId": "d39a38ed-9fb1-43be-9c58-27ff69a981b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ratio of worst to mean radius\n",
            "ratio of worst to mean texture\n",
            "ratio of worst to mean perimeter\n",
            "ratio of worst to mean area\n",
            "ratio of worst to mean smoothness\n",
            "ratio of worst to mean compactness\n",
            "ratio of worst to mean concavity\n",
            "ratio of worst to mean concave points\n",
            "ratio of worst to mean symmetry\n",
            "ratio of worst to mean fractal dimension\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "#create new 'ratio' columns\n",
        "for i in range(original_col_length - 20):\n",
        "    new_col_name = str(''.join(['ratio of ', X_columns[i+20][:5].strip(), ' to ',X_columns[i]]))\n",
        "    print(new_col_name)\n",
        "    X_columns = np.append(X_columns,new_col_name) \n",
        "    new_column_data = X[:,[i+20]] / X[:,[i]]\n",
        "    X = np.concatenate((X, new_column_data), axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Error message given as we have generated some nan values:"
      ],
      "metadata": {
        "id": "rEk4ZGk3yg4S"
      },
      "id": "rEk4ZGk3yg4S"
    },
    {
      "cell_type": "code",
      "source": [
        "np.argwhere(np.isnan(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MSHWnjsyDAA",
        "outputId": "2acbeb9b-fe38-49a2-f1d4-239259266f2f"
      },
      "id": "5MSHWnjsyDAA",
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[101,  46],\n",
              "       [101,  47],\n",
              "       [140,  46],\n",
              "       [140,  47],\n",
              "       [174,  46],\n",
              "       [174,  47],\n",
              "       [175,  46],\n",
              "       [175,  47],\n",
              "       [192,  46],\n",
              "       [192,  47],\n",
              "       [314,  46],\n",
              "       [314,  47],\n",
              "       [391,  46],\n",
              "       [391,  47],\n",
              "       [473,  46],\n",
              "       [473,  47],\n",
              "       [538,  46],\n",
              "       [538,  47],\n",
              "       [550,  46],\n",
              "       [550,  47],\n",
              "       [557,  46],\n",
              "       [557,  47],\n",
              "       [561,  46],\n",
              "       [561,  47],\n",
              "       [568,  46],\n",
              "       [568,  47]])"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We shall deal with these after the pre-processing scaling"
      ],
      "metadata": {
        "id": "kkI-rHIvypbT"
      },
      "id": "kkI-rHIvypbT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also create features from distribution of each of the mean, error and worst features respectively."
      ],
      "metadata": {
        "id": "OO_CkYl9sQQw"
      },
      "id": "OO_CkYl9sQQw"
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "id": "fe717dd5",
      "metadata": {
        "id": "fe717dd5"
      },
      "outputs": [],
      "source": [
        "# Mean error and worst features are in \n",
        "\n",
        "# mean_features = list(X.columns[:10])\n",
        "# error_features = list(X.columns[10:20])\n",
        "# worst_features = list(X.columns[20:30])\n",
        "   \n",
        "X_columns = np.append(X_columns, ['min_of_mean_features', 'mean_of_mean_features', 'max_of_mean_features',\n",
        "                                  'std_of_mean_features', 'multiple_of_mean_features', 'sum_of_mean_features',\n",
        "                                  'min_of_error_features', 'mean_of_error_features', 'max_of_error_features', \n",
        "                                  'std_of_error_features', 'multiple_of_error_features', 'sum_of_error_features', \n",
        "                                  'min_of_worst_features', 'mean_of_worst_features', 'max_of_worst_features', \n",
        "                                  'std_of_worst_features', 'multiple_of_worst_features', 'sum_of_worst_features']) \n",
        "\n",
        "for feature_ranges_data in (X[:,0:10], X[:,10:20], X[:,20:30]):\n",
        "  min_data = np.amin(feature_ranges_data, axis = 1).reshape(569,1)\n",
        "  mean_data = np.mean(feature_ranges_data, axis = 1).reshape(569,1)\n",
        "  max_data = np.amax(feature_ranges_data, axis = 1).reshape(569,1)\n",
        "  std_data = np.std(feature_ranges_data, axis = 1).reshape(569,1)\n",
        "  multiple_data = np.prod(feature_ranges_data, axis = 1).reshape(569,1)\n",
        "  sum_data = np.sum(feature_ranges_data, axis = 1).reshape(569,1)\n",
        "  new_data = np.concatenate((min_data, mean_data , max_data, std_data, multiple_data, sum_data), axis = 1)\n",
        "  X = np.concatenate((X, new_data), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "id": "1deb64c1",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1deb64c1",
        "outputId": "2bca6200-cc38-482d-94a5-84581e96acb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The new number of feature columns is now 68\n",
            " The shape of X data is now 569 rows by 68 columns\n"
          ]
        }
      ],
      "source": [
        "print(f\" The new number of feature columns is now {len(X_columns)}\")\n",
        "print(f\" The shape of X data is now {X.shape[0]} rows by {X.shape[1]} columns\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine X and y into complete 2d array for easier filtering.\n",
        "data_complete = np.concatenate((X, y.reshape(569,1)), axis = 1)\n",
        "print(f\" The shape of the complete dataset is {data_complete.shape[0]} rows by {data_complete.shape[1]} columns\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g1WWfDXFuDb",
        "outputId": "111e3165-9c46-4235-caea-f97ed2b1eac3"
      },
      "id": "8g1WWfDXFuDb",
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The shape of the complete dataset is 569 rows by 69 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create two sub arrays to contain the malignant and benign data\n",
        "malignant = data_complete[data_complete[:,-1] == 0]\n",
        "benign = data_complete[data_complete[:,-1] == 1]\n",
        "print(f\"The shape of the malignant dataset is {malignant.shape[0]} rows by {malignant.shape[1]} columns\")\n",
        "print(f\"The shape of the benign dataset is {benign.shape[0]} rows by {benign.shape[1]} columns\")\n",
        "print(f\"These match the values seen in the description:\\n{(bcd.DESCR[3085:3140])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kToIGi-BGesk",
        "outputId": "78969e84-2b52-477f-f3a6-9ac1ff56d901"
      },
      "id": "kToIGi-BGesk",
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the malignant dataset is 212 rows by 69 columns\n",
            "The shape of the benign dataset is 357 rows by 69 columns\n",
            "These match the values seen in the description:\n",
            "Class Distribution: 212 - Malignant, 357 - Benign\n",
            "\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Show distribution of each feature as boxplots.\n",
        "fig = plt.figure(figsize = (15,80))\n",
        "for index, column_name in enumerate(X_columns):\n",
        "    ax = fig.add_subplot(23,3,index + 1)\n",
        "    plt.boxplot(x = X[:,index])\n",
        "    plt.title(column_name)\n",
        "    plt.xticks([])\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "tDVjQ4GMHLyt"
      },
      "id": "tDVjQ4GMHLyt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show distribution overlap of each sub-dataframe (benign and malignant) feature as kde (kernel density plots).\n",
        "#Requires temporary pandas dataframes for kde:\n",
        "\n",
        "temp_df = pd.DataFrame(data_complete, columns = np.append(X_columns ,'target'))\n",
        "temp_malignant = temp_df[temp_df['target'] == 0]\n",
        "temp_benign = temp_df[temp_df['target'] == 1]\n",
        "fig = plt.figure(figsize = (15,80))\n",
        "for index, column_name in enumerate(X_columns):\n",
        "    ax = fig.add_subplot(23,3,index + 1)\n",
        "    temp_malignant[column_name].plot.kde(ax = ax, label = 'Malignant')\n",
        "    temp_benign[column_name].plot.kde(ax = ax, label = 'Benign')\n",
        "    plt.title(column_name)\n",
        "    if index == 0:\n",
        "        plt.legend()\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "L9bsb-hmH509"
      },
      "id": "L9bsb-hmH509",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0922c82",
      "metadata": {
        "scrolled": false,
        "id": "e0922c82"
      },
      "outputs": [],
      "source": [
        "#Linear correlations with the target\n",
        "# Positive correlation values mean a greater positive value in the column is more likely to be benign\n",
        "# Negative correlation values mean a greater positive value in the column is more likely to be malignant\n",
        "\n",
        "for index, row in (temp_df.corr()['target'].sort_values().drop('target').iteritems()):\n",
        "  index_str_length = len(index)\n",
        "  spacing = 40 - index_str_length\n",
        "  if row >= 0:\n",
        "    print(f\"{index} {' '*(spacing+1)} {row:.3f}\")\n",
        "  else:\n",
        "    print(f\"{index} {' '*(spacing)} {row:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb80c77",
      "metadata": {
        "id": "5fb80c77"
      },
      "outputs": [],
      "source": [
        "#create a list to place similar columns together (used for plotting purposes)\n",
        "similar_column_ordered = []\n",
        "for i in range(10):\n",
        "    similar_column_ordered.append(X_columns[i])\n",
        "    similar_column_ordered.append(X_columns[i+10])\n",
        "    similar_column_ordered.append(X_columns[i+20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2759c8ca",
      "metadata": {
        "scrolled": false,
        "id": "2759c8ca"
      },
      "outputs": [],
      "source": [
        "#Print distribution of benign and malignant in original feature columns\n",
        "fig = plt.figure(figsize = (15,60))\n",
        "for index, column_name in enumerate(similar_column_ordered):\n",
        "    ax = fig.add_subplot(10,3,index + 1)\n",
        "    plt.scatter(x = temp_benign.index, y = temp_benign[column_name], label = 'Benign', alpha = 0.1)\n",
        "    plt.scatter(x = temp_malignant.index, y = temp_malignant[column_name], label = 'Malignant', alpha = 0.1)\n",
        "    plt.xticks([])\n",
        "    plt.ylabel(f\"{column_name} value\")\n",
        "    plt.title(f\"{column_name}\")\n",
        "    if index == 0:\n",
        "        plt.legend()\n",
        "        plt.xlabel(f'Data given false X_values to \\n show points more clearly')\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd1490fb",
      "metadata": {
        "scrolled": false,
        "id": "bd1490fb"
      },
      "outputs": [],
      "source": [
        "#Print distribution of benign and malignant in new feature columns\n",
        "fig = plt.figure(figsize = (15,100))\n",
        "for index, column_name in enumerate(X_columns[30:]):\n",
        "    ax = fig.add_subplot(20,3,index + 1)\n",
        "    plt.scatter(x = temp_benign.index, y = temp_benign[column_name], label = 'Benign', alpha = 0.1)\n",
        "    plt.scatter(x = temp_malignant.index, y = temp_malignant[column_name], label = 'Malignant', alpha = 0.1)\n",
        "    plt.xticks([])\n",
        "    plt.ylabel(f\"{column_name} value\")\n",
        "    plt.title(f\"{column_name}\")\n",
        "    if index == 0:\n",
        "        plt.legend()\n",
        "        plt.xlabel(f'Data given false X_values to \\n show points more clearly')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce24655b",
      "metadata": {
        "id": "ce24655b"
      },
      "outputs": [],
      "source": [
        "#For comparing 2 variables calculate number of permutations:\n",
        "n_factorial = np.math.factorial(len(X_columns))\n",
        "n_minus_r_factorial = np.math.factorial(len(X_columns) - 2)\n",
        "print(f\"We have {len(X_columns)} columns.\")\n",
        "print(f\"For Comparing 2 Variables there are {int(n_factorial/ n_minus_r_factorial)} permutations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecbfcd52",
      "metadata": {
        "id": "ecbfcd52"
      },
      "source": [
        "4556 permutations means there are 4556 unique ways to compare 2 of the 68 variables.\n",
        "\n",
        "We could manually print all comparisons in scatter graphs and visually pick the features that give the best separation\n",
        "\n",
        "A more powerful method is too loop through 68 permutations (as there are 68 feature columns) at a time using a column distance to ensure no repetition of permutations.\n",
        "\n",
        "The first run will compare columns next to each other (1 column distance)\n",
        "\n",
        "The second run will compare columns two away from each other (2 column distance)\n",
        "\n",
        "And so on. Then we can generate a score based on a simple K Nearest Neighbours method. The best separation will have the highest accuracy scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "189613f9",
      "metadata": {
        "id": "189613f9"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "temp_X_df = pd.DataFrame(X, columns = X_columns))\n",
        "temp_y_df = pd.DataFrame(y, columns = 'Target')\n",
        "seen_comparisons = []\n",
        "two_variable_comparisons = pd.DataFrame({\n",
        "    'variable_1': [],\n",
        "    'variable_2': [],\n",
        "    'accuracy_score': []})\n",
        "column_length = len(X_columns)\n",
        "for column_distance in np.arange(1, column_length +1):\n",
        "\n",
        "    for index, column_name in enumerate(X_columns):\n",
        "        comparison_index = index + column_distance\n",
        "        if (comparison_index) >= column_length:\n",
        "            comparison_index = (comparison_index) - column_length\n",
        "        KNN_model = KNeighborsClassifier()\n",
        "        variable_1 = X_columns[index]\n",
        "        variable_2 = X_columns[comparison_index]\n",
        "        KNN_model.fit(temp_X_df[[variable_1, variable_2]], temp_y_df)\n",
        "        score = KNN_model.score(temp_X_df[[variable_1, variable_2]], temp_y_df)\n",
        "        \n",
        "        #Only add combination if its not already been seen.\n",
        "        if [variable_2, variable_1] not in seen_comparisons:\n",
        "            seen_comparisons.append([variable_1, variable_2])\n",
        "            two_variable_comparisons = two_variable_comparisons.append({'variable_1': variable_1,\n",
        "                                                                        'variable_2': variable_2,\n",
        "                                                                        'accuracy_score': score},\n",
        "                                                                       ignore_index = True)\n",
        "top_30 = two_variable_comparisons.sort_values(by = 'accuracy_score', ascending = False)[:30]\n",
        "top_30 = top_30.reset_index(drop=True)\n",
        "'''\n",
        "#Note the code above takes significant run time.\n",
        "#For simplicity it was saved as below to github page \n",
        "#and will be loaded from there\n",
        "'''\n",
        "top_30.to_csv('top_30.csv')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d766d467",
      "metadata": {
        "scrolled": false,
        "id": "d766d467"
      },
      "outputs": [],
      "source": [
        "top_30 = pd.read_csv('https://raw.githubusercontent.com/adbandy/data_files/main/top_30.csv', index_col = 0)\n",
        "top_30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5afe9e5",
      "metadata": {
        "scrolled": false,
        "id": "b5afe9e5"
      },
      "outputs": [],
      "source": [
        "#Visualise these top combinations\n",
        "fig = plt.figure(figsize = (15,60))\n",
        "for index in np.arange(len(top_30)):\n",
        "    variable_1 = top_30['variable_1'].iloc[index]\n",
        "    variable_2 = top_30['variable_2'].iloc[index]\n",
        "    score = top_30['accuracy_score'].iloc[index]\n",
        "    \n",
        "    ax = fig.add_subplot(10,3,index + 1)\n",
        "    plt.scatter(x = temp_malignant[variable_1], y = temp_malignant[variable_2], label = 'Malignant', alpha = 0.1)\n",
        "    plt.scatter(x = temp_benign[variable_1], y = temp_benign[variable_2], label = 'Benign', alpha = 0.1)\n",
        "    plt.xlabel(variable_1)\n",
        "    plt.ylabel(variable_2)\n",
        "    plt.title(f\"Score: {score:.4f}\")\n",
        "    if index == 0:\n",
        "        plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6af840a",
      "metadata": {
        "id": "e6af840a"
      },
      "source": [
        "Clearly our newly created features will have value in separation of clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79631a42",
      "metadata": {
        "id": "79631a42"
      },
      "source": [
        "## Feature Engineering - Standardisation\n",
        "\n",
        "First we will split the data into a training set and testing set in an (80:20 ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a90b034a",
      "metadata": {
        "scrolled": true,
        "id": "a90b034a"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "print(f\"The % proportions of classes in the splits are:\")\n",
        "print(f\"Training benign: {((len(y_train[y_train == 1]) / len(y_train)) * 100):.2f}%\")\n",
        "print(f\"Training malignant: {((len(y_train[y_train == 0]) / len(y_train)) * 100):.2f}%\\n\")\n",
        "print(f\"Testing benign: {((len(y_test[y_test == 1]) / len(y_test)) * 100):.2f}%\")\n",
        "print(f\"Testing malignant: {((len(y_test[y_test == 0]) / len(y_test)) * 100):.2f}%\")\n",
        "\n",
        "# \\ny_train :\\n{round(y_train.value_counts(normalize=True)*100,2)}\\n\\ny_test:\\n{round(y_test.value_counts(normalize=True)*100,2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see our split is sufficiently stratified."
      ],
      "metadata": {
        "id": "fTHKq8-QNuz7"
      },
      "id": "fTHKq8-QNuz7"
    },
    {
      "cell_type": "markdown",
      "id": "7cac9424",
      "metadata": {
        "id": "7cac9424"
      },
      "source": [
        "\n",
        "We must use X_train to fit a scaler. Then rescale the X_train and X_test. This is done to give each column equal importance in the modelling. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff589ff0",
      "metadata": {
        "scrolled": true,
        "id": "ff589ff0"
      },
      "outputs": [],
      "source": [
        "columns = X_columns\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = pd.DataFrame(X_train, columns = columns)\n",
        "X_test = pd.DataFrame(X_test, columns = columns)\n",
        "#Seperate scaling required for clustering as train test is meaningless here\n",
        "full_scaler = StandardScaler().fit(X)\n",
        "X = full_scaler.transform(X)\n",
        "#Display a scaled preview. \n",
        "pd.DataFrame(X, columns = columns).head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_train))\n",
        "print(type(y_train))\n",
        "print(type(X_test))\n",
        "print(type(y_test))\n",
        "#print(type(X_train_pca))\n",
        "#print(type(X_test_pca))\n",
        "#print(type(X_pca))\n",
        "#print(type(X_pca))"
      ],
      "metadata": {
        "id": "1FCZau9xDUql"
      },
      "id": "1FCZau9xDUql",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for nan values generated during standardisation\n",
        "pd.DataFrame(X, columns = columns).isna().sum().sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "KAK4P5gHXKj_"
      },
      "id": "KAK4P5gHXKj_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed earlier these values are errors generated as numpy was unable to store values so small in float 64. Having standardised the data we can safely replace these with zeros."
      ],
      "metadata": {
        "id": "5JxrSNxwPFPz"
      },
      "id": "5JxrSNxwPFPz"
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.nan_to_num(X, nan = 0.0)\n",
        "X_train = np.nan_to_num(X_train, nan = 0.0)\n",
        "X_test = np.nan_to_num(X_test, nan = 0.0)"
      ],
      "metadata": {
        "id": "coqDl5n-OSHq"
      },
      "id": "coqDl5n-OSHq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple check to show this worked.\n",
        "pd.DataFrame(X, columns = columns).isna().sum().sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "-Dxjy-QKfnQ5"
      },
      "id": "-Dxjy-QKfnQ5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#repeat for X_test:\n",
        "pd.DataFrame(X_test, columns = columns).isna().sum().sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "PcRDni5hjgn9"
      },
      "id": "PcRDni5hjgn9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#repeat for X_train:\n",
        "pd.DataFrame(X_train, columns = columns).isna().sum().sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "_9qBBdQbQK7b"
      },
      "id": "_9qBBdQbQK7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering: Dimensionallity Reduction - Using PCA\n",
        "\n",
        "Principle Component Analysis is a technique used to transform the data and determine new features that explain variance more succinctly. \n",
        "\n",
        "The mathematics is complex. The aim is to be able to reduce the feature space significantly and avoid the 'curse of dimensionality'. \n",
        "\n",
        "In essence as the number of features increases, the amount of data required to sufficiently explain it can grow exponentially."
      ],
      "metadata": {
        "id": "z6s4augzYhZy"
      },
      "id": "z6s4augzYhZy"
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_train))\n",
        "print(type(y_train))\n",
        "print(type(X_test))\n",
        "print(type(y_test))\n",
        "#print(type(X_train_pca))\n",
        "#print(type(X_test_pca))"
      ],
      "metadata": {
        "id": "C5VXM4_-_mIF"
      },
      "id": "C5VXM4_-_mIF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate the PCA and fit using the training data.\n",
        "pca = PCA()\n",
        "pca.fit(X_train)\n",
        "\n",
        "#Then transform the train and test X using this fitted PCA \n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "pca_var_perc = pca.explained_variance_ratio_ * 100\n",
        "\n",
        "# PCA all data\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "X_pca = pca.transform(X)\n",
        "\n",
        "#plot variance explained for each new feature \n",
        "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(10,8))\n",
        "ax.bar(x=[i for i in range(len(pca_var_perc))],height=pca_var_perc)\n",
        "plt.xlabel(\"Number Of Principal Components Used\")\n",
        "plt.ylabel(\"Percent Explained Variance\")\n",
        "plt.yticks(np.arange(0,50,5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sy4kmDVcYfWs"
      },
      "id": "Sy4kmDVcYfWs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display same information in value format\n",
        "pca_explained_variance = pd.DataFrame(pca.explained_variance_ratio_).head(15)\n",
        "pca_explained_variance.columns = ['% Of Explained Variance In Each Column']\n",
        "pca_explained_variance.apply(lambda x: round(x,3))"
      ],
      "metadata": {
        "id": "AfvSfoTAaBEa"
      },
      "id": "AfvSfoTAaBEa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Store as dataframe for plotting purposes\n",
        "pca_var_perc_df = pd.DataFrame(X_train_pca)\n",
        "pca_var_perc_df['target'] = y_train\n",
        "pca_var_perc_df.head()"
      ],
      "metadata": {
        "id": "Zlt_gEFGfcmh"
      },
      "id": "Zlt_gEFGfcmh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot distribution of data for two PCA variable x,y graphs.\n",
        "\n",
        "sub_df_benign = pca_var_perc_df[pca_var_perc_df['target'] == 1] \n",
        "sub_df_malignant = pca_var_perc_df[pca_var_perc_df['target'] == 0]\n",
        "\n",
        "fig = plt.figure(figsize = (10,30))\n",
        "for i in range(5):\n",
        "  for j in  range(5):\n",
        "    if (i == 0) & (j == 0):\n",
        "      fig.add_subplot(6,5, 1)\n",
        "      plt.xlabel(f\"Principal Component 0\")\n",
        "      plt.ylabel(f\"Principal Component 1\")\n",
        "\n",
        "      plt.scatter(sub_df_benign[sub_df_benign.columns[i]], sub_df_benign[sub_df_benign.columns[j+1]], alpha = 0.1, label = 'Benign')\n",
        "      plt.scatter(sub_df_malignant[sub_df_malignant.columns[i]], sub_df_malignant[sub_df_malignant.columns[j+1]], alpha = 0.1, label = 'Malignant')\n",
        "      plt.legend()\n",
        "    #Remove duplicate graphs (i.e Component 1 vs 0 is same as Component 0 vs 1)\n",
        "    elif i > (j):\n",
        "      pass\n",
        "    else:\n",
        "      fig.add_subplot(6,5, ((i*5)+(j+1)))\n",
        "      plt.xlabel(f\"Principal Component {i}\")\n",
        "      plt.ylabel(f\"Principal Component {j+1}\")\n",
        "\n",
        "      plt.scatter(sub_df_benign[sub_df_benign.columns[i]], sub_df_benign[sub_df_benign.columns[j+1]], alpha = 0.1, label = 'Benign')\n",
        "      plt.scatter(sub_df_malignant[sub_df_malignant.columns[i]], sub_df_malignant[sub_df_malignant.columns[j+1]], alpha = 0.1, label = 'Malignant')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "tDslOd-ljkLk"
      },
      "id": "tDslOd-ljkLk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering - Dimensionality Reduction- K-PCA\n",
        "\n",
        "Kernel PCA is another similar technique compared to PCA but can work better on more complex relationships."
      ],
      "metadata": {
        "id": "rrqDudbODC0b"
      },
      "id": "rrqDudbODC0b"
    },
    {
      "cell_type": "code",
      "source": [
        "#Kernel PCA, Instantiate, fit and transform data.\n",
        "\n",
        "k_pca = KernelPCA()\n",
        "k_pca.fit(X_train)\n",
        "X_train_k_pca = k_pca.transform(X_train)\n",
        "X_test_k_pca = k_pca.transform(X_test)"
      ],
      "metadata": {
        "id": "TElahi8zb0qV"
      },
      "id": "TElahi8zb0qV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Store as dataframe for plotting purposes\n",
        "k_pca_var_perc_df = pd.DataFrame(X_train_k_pca)\n",
        "k_pca_var_perc_df['target'] = y_train\n",
        "k_pca_var_perc_df.head()"
      ],
      "metadata": {
        "id": "4n6BqvPQnlts"
      },
      "id": "4n6BqvPQnlts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot distribution of data for two K-PCA variable x,y graphs.\n",
        "\n",
        "\n",
        "sub_df_benign = k_pca_var_perc_df[k_pca_var_perc_df['target'] == 1] \n",
        "sub_df_malignant = k_pca_var_perc_df[k_pca_var_perc_df['target'] == 0]\n",
        "\n",
        "fig = plt.figure(figsize = (10,30))\n",
        "for i in range(5):\n",
        "  for j in  range(5):\n",
        "    if (i == 0) & (j == 0):\n",
        "      fig.add_subplot(6,5, 1)\n",
        "      plt.xlabel(f\"Principal Component 0\")\n",
        "      plt.ylabel(f\"Principal Component 1\")\n",
        "\n",
        "      plt.scatter(sub_df_benign[sub_df_benign.columns[i]], sub_df_benign[sub_df_benign.columns[j+1]], alpha = 0.1, label = 'Benign')\n",
        "      plt.scatter(sub_df_malignant[sub_df_malignant.columns[i]], sub_df_malignant[sub_df_malignant.columns[j+1]], alpha = 0.1, label = 'Malignant')\n",
        "      plt.legend()\n",
        "    #Remove duplicate graphs (i.e Component 1 vs 0 is same as Component 0 vs 1)\n",
        "    elif i > (j):\n",
        "      pass\n",
        "    else:\n",
        "      fig.add_subplot(6,5, ((i*5)+(j+1)))\n",
        "      plt.xlabel(f\"Principal Component {i}\")\n",
        "      plt.ylabel(f\"Principal Component {j+1}\")\n",
        "\n",
        "      plt.scatter(sub_df_benign[sub_df_benign.columns[i]], sub_df_benign[sub_df_benign.columns[j+1]], alpha = 0.1, label = 'Benign')\n",
        "      plt.scatter(sub_df_malignant[sub_df_malignant.columns[i]], sub_df_malignant[sub_df_malignant.columns[j+1]], alpha = 0.1, label = 'Malignant')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "RLWzx16PnwNs"
      },
      "id": "RLWzx16PnwNs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering - Comparing PCA with K_PCA\n",
        "Visually these two methods appear to show similar results.\n",
        "\n",
        "Below is a replot of the first rows from each graphic."
      ],
      "metadata": {
        "id": "Q1FyMm-goIeu"
      },
      "id": "Q1FyMm-goIeu"
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot first row of each graphic (PCA and K-PCA)\n",
        "\n",
        "sub_df_benign_pca = k_pca_var_perc_df[k_pca_var_perc_df['target'] == 1] \n",
        "sub_df_malignant_pca = k_pca_var_perc_df[k_pca_var_perc_df['target'] == 0]\n",
        "\n",
        "sub_df_benign_k_pca = k_pca_var_perc_df[k_pca_var_perc_df['target'] == 1] \n",
        "sub_df_malignant_k_pca = k_pca_var_perc_df[k_pca_var_perc_df['target'] == 0]\n",
        "\n",
        "fig = plt.figure(figsize = (10,30))\n",
        "\n",
        "for i in [0]:\n",
        "  for j in  range(5):\n",
        "    if (i == 0) & (j == 0):\n",
        "      fig.add_subplot(6,5, 1)\n",
        "      plt.xlabel(f\"PCA Component 0\")\n",
        "      plt.ylabel(f\"PCA Component 1\")\n",
        "\n",
        "      plt.scatter(sub_df_benign_pca[sub_df_benign_pca.columns[i]], sub_df_benign_pca[sub_df_benign_pca.columns[j+1]], alpha = 0.1, label = 'Benign')\n",
        "      plt.scatter(sub_df_malignant_pca[sub_df_malignant_pca.columns[i]], sub_df_malignant_pca[sub_df_malignant_pca.columns[j+1]], alpha = 0.1, label = 'Malignant')\n",
        "      plt.legend()\n",
        "    else:\n",
        "      fig.add_subplot(6,5, ((i*5)+(j+1)))\n",
        "      plt.xlabel(f\"PCA Component {i}\")\n",
        "      plt.ylabel(f\"PCA Component {j+1}\")\n",
        "\n",
        "      plt.scatter(sub_df_benign_pca[sub_df_benign_pca.columns[i]], sub_df_benign_pca[sub_df_benign_pca.columns[j+1]], alpha = 0.1, label = 'Benign')\n",
        "      plt.scatter(sub_df_malignant_pca[sub_df_malignant_pca.columns[i]], sub_df_malignant_pca[sub_df_malignant_pca.columns[j+1]], alpha = 0.1, label = 'Malignant')     \n",
        "\n",
        "\n",
        "for i in [0]:\n",
        "  for j in  range(5):\n",
        "    if (i == 0) & (j == 0):\n",
        "      fig.add_subplot(6,5, 6)\n",
        "      plt.xlabel(f\"K_PCA Component 0\")\n",
        "      plt.ylabel(f\"K_PCA Component 1\")\n",
        "\n",
        "      plt.scatter(sub_df_benign_k_pca[sub_df_benign_k_pca.columns[i]], sub_df_benign_k_pca[sub_df_benign_k_pca.columns[j+1]], alpha = 0.1, label = 'Benign')\n",
        "      plt.scatter(sub_df_malignant_k_pca[sub_df_malignant_k_pca.columns[i]], sub_df_malignant_k_pca[sub_df_malignant.columns[j+1]], alpha = 0.1, label = 'Malignant')\n",
        "      plt.legend()\n",
        "    else:\n",
        "      fig.add_subplot(6,5, (5+j+1))\n",
        "      plt.xlabel(f\"K_PCA Component {i}\")\n",
        "      plt.ylabel(f\"K_PCA Component {j+1}\")\n",
        "\n",
        "      plt.scatter(sub_df_benign_k_pca[sub_df_benign_k_pca.columns[i]], sub_df_benign_k_pca[sub_df_benign_k_pca.columns[j+1]], alpha = 0.1, label = 'Benign')\n",
        "      plt.scatter(sub_df_malignant_k_pca[sub_df_malignant_k_pca.columns[i]], sub_df_malignant_k_pca[sub_df_malignant.columns[j+1]], alpha = 0.1, label = 'Malignant')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "YoDUsBbxoGwA"
      },
      "id": "YoDUsBbxoGwA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No discernable differences between PCA and k_PCA"
      ],
      "metadata": {
        "id": "SsKmUq4Fpjjz"
      },
      "id": "SsKmUq4Fpjjz"
    },
    {
      "cell_type": "code",
      "source": [
        "#Having completed feature engineering we can also store X as a dataframe for plotting purposes later.\n",
        "X = pd.DataFrame(X, columns = X_columns)\n",
        "y = pd.Series(y)"
      ],
      "metadata": {
        "id": "vDnvJkeU5xGf"
      },
      "id": "vDnvJkeU5xGf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#And store train test splits as dataframes for plotting purposes later.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "print(f\"The % proportions of classes in the splits are:\")\n",
        "print(f\"Training benign: {((len(y_train[y_train == 1]) / len(y_train)) * 100):.2f}%\")\n",
        "print(f\"Training malignant: {((len(y_train[y_train == 0]) / len(y_train)) * 100):.2f}%\\n\")\n",
        "print(f\"Testing benign: {((len(y_test[y_test == 1]) / len(y_test)) * 100):.2f}%\")\n",
        "print(f\"Testing malignant: {((len(y_test[y_test == 0]) / len(y_test)) * 100):.2f}%\")"
      ],
      "metadata": {
        "id": "VZzmqu6m8_iT"
      },
      "id": "VZzmqu6m8_iT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X))\n",
        "print(type(X_pca))\n",
        "\n",
        "print(type(X_train))\n",
        "print(type(X_test))\n",
        "print(type(X_pca))\n",
        "print(type(y))"
      ],
      "metadata": {
        "id": "VybSYH7d2Uxg"
      },
      "id": "VybSYH7d2Uxg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_all = X.to_numpy()\n",
        "y_all = y.to_numpy()\n",
        "\n",
        "X_train_np = X_train.to_numpy()\n",
        "X_test_np = X_test.to_numpy()\n",
        "\n",
        "y_train_np = y_train.to_numpy()\n",
        "y_test_np = y_test.to_numpy()"
      ],
      "metadata": {
        "id": "qtIHq3mU2Zk4"
      },
      "id": "qtIHq3mU2Zk4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_all))\n",
        "print(type(X_pca))\n",
        "\n",
        "print(type(X_train_np))\n",
        "print(type(X_test_np))\n",
        "\n",
        "print(type(X_pca))\n",
        "print(type(y_all))\n",
        "\n",
        "print(type(X_train_pca))\n",
        "print(type(X_test_pca))"
      ],
      "metadata": {
        "id": "bqhW2NlH2rA4"
      },
      "id": "bqhW2NlH2rA4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II Application: Clustering"
      ],
      "metadata": {
        "id": "6pfG7o9b4upS"
      },
      "id": "6pfG7o9b4upS"
    },
    {
      "cell_type": "markdown",
      "id": "f3bbef5d",
      "metadata": {
        "id": "f3bbef5d"
      },
      "source": [
        "## Spectral Clustering\n",
        "In general this technique is most suitable for when the data does not form seperate spherical like clusters with little overlap.\n",
        "For instance it works well for sphere within a hollow sphere or double helix style structure.\n",
        "\n",
        "Whether this method is applicable is difficult for multi-dimensional space, but from seeing our top_30 comparisons (of one variable vs another) we can say some combinations are potentially spherical-like and separable."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let us review a simplistic model. And test it"
      ],
      "metadata": {
        "id": "MoGOvci_Rt14"
      },
      "id": "MoGOvci_Rt14"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b741027",
      "metadata": {
        "scrolled": true,
        "id": "2b741027"
      },
      "outputs": [],
      "source": [
        "spectral_cluster_model = SpectralClustering(n_clusters = 2, random_state = 1, n_init = 10, affinity = 'nearest_neighbors', assign_labels= 'kmeans')\n",
        "spectral_cluster_model.fit(X_all)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_all))\n",
        "print(type(X_pca))\n",
        "\n",
        "print(type(X_train_np))\n",
        "print(type(X_test_np))\n",
        "\n",
        "print(type(X_pca))\n",
        "print(type(y_all))\n",
        "\n",
        "print(type(X_train_pca))\n",
        "print(type(X_test_pca))"
      ],
      "metadata": {
        "id": "a4iqxgfa2YJD"
      },
      "id": "a4iqxgfa2YJD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "297aafa6",
      "metadata": {
        "id": "297aafa6"
      },
      "outputs": [],
      "source": [
        "clustering_performance_evaluation(X_all, spectral_cluster_model.labels_, print_output = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "High Score is desirable for Calinski-Harabasz Score.\n",
        "The best Silhouette score a model can obtain is 1 with 0 being the worst."
      ],
      "metadata": {
        "id": "2lqjdbaLqZz8"
      },
      "id": "2lqjdbaLqZz8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ea0743c",
      "metadata": {
        "scrolled": true,
        "id": "0ea0743c"
      },
      "outputs": [],
      "source": [
        "simple_scores_spectral_clustering = classification_using_clustering_performance_evaluation(y, spectral_cluster_model.labels_, print_output = True, classification= False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In all of these classifications scores, higher values are better."
      ],
      "metadata": {
        "id": "1gWfJgO7qotU"
      },
      "id": "1gWfJgO7qotU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spectral Clustering - Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "35TOPEuHPSzU"
      },
      "id": "35TOPEuHPSzU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b22e7e0a",
      "metadata": {
        "id": "b22e7e0a"
      },
      "outputs": [],
      "source": [
        "#Create a dataframe for storing scores and parameters used.\n",
        "spectral_clustering_hyperparameter_tuning_df = pd.DataFrame({'n_init': [],\n",
        "                                                             'eigen_solver': [],\n",
        "                                                             'n_neigbors': [],\n",
        "                                                             'Homogeneity Score': [],\n",
        "                                                             'Completeness Score': [],\n",
        "                                                             'V_measure Score': [],\n",
        "                                                             'Accuracy Score': [],\n",
        "                                                             'Balanced Accuracy Score': [],\n",
        "                                                             'F1 Score': [],\n",
        "                                                             'Adjusted_rand_score': [],\n",
        "                                                             'Silhouette Score': [],\n",
        "                                                             'Calinski-Harabasz Score': [],\n",
        "                                                             'David-Bouldin Score': []})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d3d261",
      "metadata": {
        "id": "69d3d261"
      },
      "outputs": [],
      "source": [
        "for n_init in [10,100,1000]:\n",
        "    for eigen_solver in ['arpack', 'lobpcg']:\n",
        "        for n_neigbors in [1,10,100]:\n",
        "            spectral_cluster_model= SpectralClustering(n_clusters = 2,\n",
        "                                                       random_state = 1,\n",
        "                                                       affinity = 'nearest_neighbors',\n",
        "                                                       assign_labels= 'kmeans',\n",
        "                                                       n_init = n_init,\n",
        "                                                      eigen_solver= eigen_solver,\n",
        "                                                      n_neighbors= n_neigbors)\n",
        "            spectral_cluster_model.fit(X_all)\n",
        "            h,c,v,a,b,f1,ars = classification_using_clustering_performance_evaluation(y_all, spectral_cluster_model.labels_ , classification= False)\n",
        "            ss, chs, dbs =  clustering_performance_evaluation(X_all, spectral_cluster_model.labels_)\n",
        "            spectral_clustering_hyperparameter_tuning_df = \\\n",
        "                spectral_clustering_hyperparameter_tuning_df.append({'n_init': n_init,\n",
        "                                                                     'eigen_solver': eigen_solver,\n",
        "                                                                     'n_neigbors': n_neigbors,\n",
        "                                                                     'Homogeneity Score': h,\n",
        "                                                                     'Completeness Score': c,\n",
        "                                                                     'V_measure Score': v,\n",
        "                                                                     'Accuracy Score': a,\n",
        "                                                                     'Balanced Accuracy Score': b,\n",
        "                                                                     'F1 Score': f1,\n",
        "                                                                     'Adjusted_rand_score': ars,\n",
        "                                                                     'Silhouette Score': ss,\n",
        "                                                                     'Calinski-Harabasz Score': chs,\n",
        "                                                                     'David-Bouldin Score': dbs}, ignore_index = True)                                                             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "428d54fd",
      "metadata": {
        "scrolled": false,
        "id": "428d54fd"
      },
      "outputs": [],
      "source": [
        "spectral_clustering_hyperparameter_tuning_df.sort_values(by = 'Adjusted_rand_score', ascending = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e947d7e",
      "metadata": {
        "id": "9e947d7e"
      },
      "source": [
        "Higher n_neigbhours is most influential here. Will experiment with high neighbour count. Other hyperparameter tuning has had little effect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7d32957",
      "metadata": {
        "id": "b7d32957"
      },
      "outputs": [],
      "source": [
        "#Create a dataframe for storing scores and parameters used.\n",
        "spectral_clustering_hyperparameter_tuning_n_neighbours_df = pd.DataFrame({'n_init': [],\n",
        "                                                             'eigen_solver': [],\n",
        "                                                             'n_neigbors': [],\n",
        "                                                             'Homogeneity Score': [],\n",
        "                                                             'Completeness Score': [],\n",
        "                                                             'V_measure Score': [],\n",
        "                                                             'Accuracy Score': [],\n",
        "                                                             'Balanced Accuracy Score': [],\n",
        "                                                             'F1 Score': [],\n",
        "                                                             'Adjusted_rand_score': [],\n",
        "                                                             'Silhouette Score': [],\n",
        "                                                             'Calinski-Harabasz Score': [],\n",
        "                                                             'David-Bouldin Score': []})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbfa37fc",
      "metadata": {
        "id": "fbfa37fc"
      },
      "outputs": [],
      "source": [
        "for n_init in [10]:\n",
        "    for eigen_solver in ['arpack']:\n",
        "        \n",
        "        for n_neigbors in list(np.arange(50,550, 10)):\n",
        "            spectral_cluster_model= SpectralClustering(n_clusters = 2,\n",
        "                                                       random_state = 1,\n",
        "                                                       affinity = 'nearest_neighbors',\n",
        "                                                       assign_labels= 'kmeans',\n",
        "                                                       n_init = n_init,\n",
        "                                                      eigen_solver= eigen_solver,\n",
        "                                                      n_neighbors= n_neigbors)\n",
        "            spectral_cluster_model.fit(X_all)\n",
        "            h,c,v,a,b,f1,ars = classification_using_clustering_performance_evaluation(y_all, spectral_cluster_model.labels_, classification= False)\n",
        "            ss, chs, dbs =  clustering_performance_evaluation(X_all, spectral_cluster_model.labels_)\n",
        "            spectral_clustering_hyperparameter_tuning_n_neighbours_df = \\\n",
        "                spectral_clustering_hyperparameter_tuning_n_neighbours_df.append({'n_init': n_init,\n",
        "                                                                     'eigen_solver': eigen_solver,\n",
        "                                                                     'n_neigbors': n_neigbors,\n",
        "                                                                     'Homogeneity Score': h,\n",
        "                                                                     'Completeness Score': c,\n",
        "                                                                     'V_measure Score': v,\n",
        "                                                                     'Accuracy Score': a,\n",
        "                                                                     'Balanced Accuracy Score': b,\n",
        "                                                                     'F1 Score': f1,\n",
        "                                                                     'Adjusted_rand_score': ars,\n",
        "                                                                     'Silhouette Score': ss,\n",
        "                                                                     'Calinski-Harabasz Score': chs,\n",
        "                                                                     'David-Bouldin Score': dbs}, ignore_index = True)                                                             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0ae5250",
      "metadata": {
        "scrolled": false,
        "id": "a0ae5250"
      },
      "outputs": [],
      "source": [
        "spectral_clustering_hyperparameter_tuning_n_neighbours_df = spectral_clustering_hyperparameter_tuning_n_neighbours_df.sort_values(by = 'Adjusted_rand_score', ascending = False)\n",
        "spectral_clustering_hyperparameter_tuning_n_neighbours_df[['Homogeneity Score',\n",
        "       'Completeness Score', 'V_measure Score', 'Accuracy Score',\n",
        "       'Balanced Accuracy Score', 'F1 Score', 'Adjusted_rand_score']] = spectral_clustering_hyperparameter_tuning_n_neighbours_df[['Homogeneity Score',\n",
        "       'Completeness Score', 'V_measure Score', 'Accuracy Score',\n",
        "       'Balanced Accuracy Score', 'F1 Score', 'Adjusted_rand_score']].apply(lambda x: round(x,3))\n",
        "spectral_clustering_hyperparameter_tuning_n_neighbours_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dae933d",
      "metadata": {
        "id": "5dae933d"
      },
      "outputs": [],
      "source": [
        "spectral_clustering_hyperparameter_tuning_n_neighbours_df.sort_values(by = 'Adjusted_rand_score', ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69a743dc",
      "metadata": {
        "id": "69a743dc"
      },
      "outputs": [],
      "source": [
        "#Sort by n_neighbours for plotting purposes\n",
        "spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted = spectral_clustering_hyperparameter_tuning_n_neighbours_df.sort_values(by= 'n_neigbors')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78e734ee",
      "metadata": {
        "scrolled": false,
        "id": "78e734ee"
      },
      "outputs": [],
      "source": [
        "#plot the various scores\n",
        "fig = plt.figure(figsize = (10,40))\n",
        "for i,j in zip(range(1,11), ['Homogeneity Score',\n",
        "       'Completeness Score', 'V_measure Score', 'Accuracy Score',\n",
        "       'Balanced Accuracy Score', 'F1 Score', 'Adjusted_rand_score',\n",
        "       'Silhouette Score', 'Calinski-Harabasz Score', 'David-Bouldin Score']):\n",
        "    fig.add_subplot(5,2,i)\n",
        "    plt.plot(spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted['n_neigbors'],\n",
        "            spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted[j])\n",
        "    plt.title(f\"{j}\\n Vs Number Of Neigbors Clustering\")\n",
        "    plt.xlabel(f\"Number Of Neigbors\")\n",
        "    plt.ylabel(f\"{j}\")\n",
        "    if i <= 7:\n",
        "        plt.axvline(130, linestyle = 'dashed', alpha = 0.2)\n",
        "        y_value_for_text = ((np.max(spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted[j]) - np.min(spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted[j])) / 2) + np.min(spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted[j])\n",
        "        plt.text(140,y_value_for_text, s='130', rotation = 90, c= '#1f77b4', alpha = 0.5)\n",
        "    if i >7:\n",
        "        plt.axvline(540, linestyle = 'dashed', alpha = 0.2)\n",
        "        y_value_for_text = ((np.max(spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted[j]) - np.min(spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted[j])) / 2) + np.min(spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted[j])\n",
        "        plt.text(520,y_value_for_text, s='540', rotation = 90, c= '#1f77b4', alpha = 0.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c57cb7f",
      "metadata": {
        "id": "0c57cb7f"
      },
      "source": [
        "Best overall classification metrics are given at 130 Neighbours. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd706c0",
      "metadata": {
        "id": "abd706c0"
      },
      "outputs": [],
      "source": [
        "best_score_ss = spectral_clustering_hyperparameter_tuning_n_neighbours_df.sort_values(by= 'Silhouette Score', ascending = False).iloc[0]['n_neigbors']\n",
        "best_score_chs = spectral_clustering_hyperparameter_tuning_n_neighbours_df.sort_values(by= 'Calinski-Harabasz Score', ascending = False).iloc[0]['n_neigbors']\n",
        "best_score_dbs = spectral_clustering_hyperparameter_tuning_n_neighbours_df.sort_values(by= 'David-Bouldin Score', ascending = True).iloc[0]['n_neigbors']\n",
        "\n",
        "print(f\"N_neigbors that gave the best clustering metrics: \\\n",
        "      \\nFor Silhouette Score : {int(best_score_ss)} \\\n",
        "      \\nFor Calinski-Harabasz Score : {int(best_score_chs)} \\\n",
        "      \\nFor David-Bouldin Score : {int(best_score_dbs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next lets compare these values with those obtained using the PCA reduced dimensionality data\n",
        "\n",
        "We will use only the top 7 columns from PCA"
      ],
      "metadata": {
        "id": "VWXmLouBzsbt"
      },
      "id": "VWXmLouBzsbt"
    },
    {
      "cell_type": "code",
      "source": [
        "X_pca = X_pca[:,:6]"
      ],
      "metadata": {
        "id": "61VNFgo11T_e"
      },
      "id": "61VNFgo11T_e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dataframe for storing scores and parameters used.\n",
        "pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df = pd.DataFrame({'n_init': [],\n",
        "                                                             'eigen_solver': [],\n",
        "                                                             'n_neigbors': [],\n",
        "                                                             'Homogeneity Score': [],\n",
        "                                                             'Completeness Score': [],\n",
        "                                                             'V_measure Score': [],\n",
        "                                                             'Accuracy Score': [],\n",
        "                                                             'Balanced Accuracy Score': [],\n",
        "                                                             'F1 Score': [],\n",
        "                                                             'Adjusted_rand_score': [],\n",
        "                                                             'Silhouette Score': [],\n",
        "                                                             'Calinski-Harabasz Score': [],\n",
        "                                                             'David-Bouldin Score': []})"
      ],
      "metadata": {
        "id": "eJmC3ZGn1n0W"
      },
      "id": "eJmC3ZGn1n0W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n_init in [10]:\n",
        "    for eigen_solver in ['arpack']:\n",
        "        \n",
        "        for n_neigbors in list(np.arange(50,550, 10)):\n",
        "            pca_spectral_cluster_model= SpectralClustering(n_clusters = 2,\n",
        "                                                       random_state = 1,\n",
        "                                                       affinity = 'nearest_neighbors',\n",
        "                                                       assign_labels= 'kmeans',\n",
        "                                                       n_init = n_init,\n",
        "                                                      eigen_solver= eigen_solver,\n",
        "                                                      n_neighbors= n_neigbors)\n",
        "            pca_spectral_cluster_model.fit(X_pca[:,:6])\n",
        "            h,c,v,a,b,f1,ars = classification_using_clustering_performance_evaluation(y_all, pca_spectral_cluster_model.labels_, classification= False)\n",
        "            ss, chs, dbs =  clustering_performance_evaluation(X_all, pca_spectral_cluster_model.labels_)\n",
        "            pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df = \\\n",
        "                pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df.append({'n_init': n_init,\n",
        "                                                                     'eigen_solver': eigen_solver,\n",
        "                                                                     'n_neigbors': n_neigbors,\n",
        "                                                                     'Homogeneity Score': h,\n",
        "                                                                     'Completeness Score': c,\n",
        "                                                                     'V_measure Score': v,\n",
        "                                                                     'Accuracy Score': a,\n",
        "                                                                     'Balanced Accuracy Score': b,\n",
        "                                                                     'F1 Score': f1,\n",
        "                                                                     'Adjusted_rand_score': ars,\n",
        "                                                                     'Silhouette Score': ss,\n",
        "                                                                     'Calinski-Harabasz Score': chs,\n",
        "                                                                     'David-Bouldin Score': dbs}, ignore_index = True)"
      ],
      "metadata": {
        "id": "czXoeX0yz0Nv"
      },
      "id": "czXoeX0yz0Nv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df.sort_values(by= 'Adjusted_rand_score', ascending = False )"
      ],
      "metadata": {
        "id": "WOPu452l2e_7"
      },
      "id": "WOPu452l2e_7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted = pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df.sort_values(by= 'n_neigbors')\n",
        "fig = plt.figure(figsize = (10,40))\n",
        "for i,j in zip(range(1,11), ['Homogeneity Score',\n",
        "       'Completeness Score', 'V_measure Score', 'Accuracy Score',\n",
        "       'Balanced Accuracy Score', 'F1 Score', 'Adjusted_rand_score',\n",
        "       'Silhouette Score', 'Calinski-Harabasz Score', 'David-Bouldin Score']):\n",
        "    fig.add_subplot(5,2,i)\n",
        "    plt.plot(pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted['n_neigbors'],\n",
        "            pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted[j])\n",
        "    plt.title(f\"{j}\\n Vs Number Of Neigbors Clustering\")\n",
        "    plt.xlabel(f\"Number Of Neigbors\")\n",
        "    plt.ylabel(f\"{j}\")\n",
        "    if i <= 7:\n",
        "        plt.axvline(140, linestyle = 'dashed', alpha = 0.2)\n",
        "        y_value_for_text = ((np.max(pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted[j]) - np.min(pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted[j])) / 2) + np.min(pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted[j])\n",
        "        plt.text(150,y_value_for_text, s='140', rotation = 90, c= '#1f77b4', alpha = 0.5)\n",
        "    if i >7:\n",
        "        plt.axvline(540, linestyle = 'dashed', alpha = 0.2)\n",
        "        y_value_for_text = ((np.max(pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted[j]) - np.min(pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted[j])) / 2) + np.min(pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df_sorted[j])\n",
        "        plt.text(520,y_value_for_text, s='540', rotation = 90, c= '#1f77b4', alpha = 0.5)"
      ],
      "metadata": {
        "id": "-cp95Cxz1783"
      },
      "id": "-cp95Cxz1783",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best classification metrics at 140 neighbors"
      ],
      "metadata": {
        "id": "XLrAXaPhQZNF"
      },
      "id": "XLrAXaPhQZNF"
    },
    {
      "cell_type": "code",
      "source": [
        "best_score_ss = pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df.sort_values(by= 'Silhouette Score', ascending = False).iloc[0]['n_neigbors']\n",
        "best_score_chs = pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df.sort_values(by= 'Calinski-Harabasz Score', ascending = False).iloc[0]['n_neigbors']\n",
        "best_score_dbs = pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df.sort_values(by= 'David-Bouldin Score', ascending = True).iloc[0]['n_neigbors']\n",
        "\n",
        "print(f\"N_neigbors that gave the best clustering metrics: \\\n",
        "      \\nFor Silhouette Score : {int(best_score_ss)} \\\n",
        "      \\nFor Calinski-Harabasz Score : {int(best_score_chs)} \\\n",
        "      \\nFor David-Bouldin Score : {int(best_score_dbs)}\")"
      ],
      "metadata": {
        "id": "smpBoPD90t0r"
      },
      "id": "smpBoPD90t0r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing the Spectral Clustering and PCA- Spectral Clustering:"
      ],
      "metadata": {
        "id": "BTPQ1kzlD3yn"
      },
      "id": "BTPQ1kzlD3yn"
    },
    {
      "cell_type": "code",
      "source": [
        "spectral_comparisons = pd.concat([pd.DataFrame(spectral_clustering_hyperparameter_tuning_n_neighbours_df.sort_values(by = 'Adjusted_rand_score',ascending = False).iloc[0]),\n",
        "          pd.DataFrame(pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df.sort_values(by = 'Adjusted_rand_score',ascending = False).iloc[0])], axis = 1)\n",
        "spectral_comparisons.columns = ['Spectral Results', 'PCA Spectral Results'] \n",
        "spectral_comparisons"
      ],
      "metadata": {
        "id": "tOTNLN8F22-3"
      },
      "id": "tOTNLN8F22-3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA Spectral Clustering is giving slightly worse results in terms of the classification and clustering metrics."
      ],
      "metadata": {
        "id": "yhntXM2F4MXK"
      },
      "id": "yhntXM2F4MXK"
    },
    {
      "cell_type": "markdown",
      "id": "5b75cb81",
      "metadata": {
        "id": "5b75cb81"
      },
      "source": [
        "## Agglomerative Clustering\n",
        "\n",
        "Lets see some base scores before hyper parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94395a90",
      "metadata": {
        "scrolled": true,
        "id": "94395a90"
      },
      "outputs": [],
      "source": [
        "agglomerative_cluster_model = AgglomerativeClustering(n_clusters = 2)\n",
        "agglomerative_cluster_model.fit(X_all)\n",
        "clustering_performance_evaluation\n",
        "classification_using_clustering_performance_evaluation(y_all, agglomerative_cluster_model.labels_, print_output = True, classification= False)\n",
        "clustering_performance_evaluation(X_all, agglomerative_cluster_model.labels_, print_output = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "257b5bc1",
      "metadata": {
        "id": "257b5bc1"
      },
      "outputs": [],
      "source": [
        "#Create a dataframe for storing scores and parameters used.\n",
        "agglomerative_cluster_hyperparameter_tuning_df = pd.DataFrame({'affinity': [],\n",
        "                                                             'linkage': [],\n",
        "                                                             'Homogeneity Score': [],\n",
        "                                                             'Completeness Score': [],\n",
        "                                                             'V_measure Score': [],\n",
        "                                                             'Accuracy Score': [],\n",
        "                                                             'Balanced Accuracy Score': [],\n",
        "                                                             'F1 Score': [],\n",
        "                                                             'Adjusted_rand_score': [],\n",
        "                                                             'Silhouette Score': [],\n",
        "                                                             'Calinski-Harabasz Score': [],\n",
        "                                                             'David-Bouldin Score': []})\n",
        "for affinity in ['euclidean', 'l1', 'l2', 'manhattan', 'cosine']:\n",
        "    for linkage in ['ward', 'average', 'complete', 'single']:\n",
        "        #ward only works with euclidean so pass on all non euclidean affinities\n",
        "        if (linkage == 'ward') & np.any([affinity == 'l1' or\n",
        "                                      affinity == 'l2' or\n",
        "                                      affinity == 'manhattan' or\n",
        "                                      affinity == 'cosine']):\n",
        "            pass\n",
        "        else:\n",
        "            agglomerative_cluster_model= AgglomerativeClustering(n_clusters = 2,\n",
        "                                                       affinity = affinity,\n",
        "                                                       linkage = linkage)\n",
        "            agglomerative_cluster_model.fit(X_all)\n",
        "            h,c,v,a,b,f1,ars = classification_using_clustering_performance_evaluation(y_all, agglomerative_cluster_model.labels_ , classification= False)\n",
        "            ss, chs, dbs =  clustering_performance_evaluation(X_all, agglomerative_cluster_model.labels_)\n",
        "            agglomerative_cluster_hyperparameter_tuning_df = \\\n",
        "                agglomerative_cluster_hyperparameter_tuning_df.append({'affinity': affinity,\n",
        "                                                                     'linkage': linkage,\n",
        "                                                                     'Homogeneity Score': h,\n",
        "                                                                     'Completeness Score': c,\n",
        "                                                                     'V_measure Score': v,\n",
        "                                                                     'Accuracy Score': a,\n",
        "                                                                     'Balanced Accuracy Score': b,\n",
        "                                                                     'F1 Score': f1,\n",
        "                                                                     'Adjusted_rand_score': ars,\n",
        "                                                                     'Silhouette Score': ss,\n",
        "                                                                     'Calinski-Harabasz Score': chs,\n",
        "                                                                     'David-Bouldin Score': dbs}, ignore_index = True)                                                             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411895c9",
      "metadata": {
        "scrolled": true,
        "id": "411895c9"
      },
      "outputs": [],
      "source": [
        "agglomerative_cluster_hyperparameter_tuning_df = agglomerative_cluster_hyperparameter_tuning_df.sort_values(by= 'Adjusted_rand_score', ascending = False)\n",
        "agglomerative_cluster_hyperparameter_tuning_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning has produced significantly better scores."
      ],
      "metadata": {
        "id": "POxMgXoI5Iyp"
      },
      "id": "POxMgXoI5Iyp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e525d0c",
      "metadata": {
        "id": "8e525d0c"
      },
      "outputs": [],
      "source": [
        "#combine parameters to one name\n",
        "agglomerative_cluster_hyperparameter_tuning_df['type'] = agglomerative_cluster_hyperparameter_tuning_df[['affinity','linkage']].apply(' '.join, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fee4f04",
      "metadata": {
        "scrolled": false,
        "id": "2fee4f04"
      },
      "outputs": [],
      "source": [
        "#plot the various scores\n",
        "fig = plt.figure(figsize = (10,40))\n",
        "for i,j in zip(range(1,11), ['Homogeneity Score',\n",
        "       'Completeness Score', 'V_measure Score', 'Accuracy Score',\n",
        "       'Balanced Accuracy Score', 'F1 Score', 'Adjusted_rand_score',\n",
        "       'Silhouette Score', 'Calinski-Harabasz Score', 'David-Bouldin Score']):\n",
        "    fig.add_subplot(5,2,i)\n",
        "    plt.plot(agglomerative_cluster_hyperparameter_tuning_df['type'],\n",
        "            agglomerative_cluster_hyperparameter_tuning_df[j])\n",
        "    plt.title(f\"{j}\\n Vs Agglomerative Hyperparameters\")\n",
        "    plt.xlabel(f\"Hyperparameters\")\n",
        "    plt.xticks(rotation = 90)\n",
        "    plt.ylabel(f\"{j}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best Classification Parameters overall is for {agglomerative_cluster_hyperparameter_tuning_df['type'].iloc[0]}\")"
      ],
      "metadata": {
        "id": "ATbF491U5yIN"
      },
      "id": "ATbF491U5yIN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71bff12c",
      "metadata": {
        "id": "71bff12c"
      },
      "outputs": [],
      "source": [
        "best_score_ss = agglomerative_cluster_hyperparameter_tuning_df.sort_values(by= 'Silhouette Score', ascending = False).iloc[0]['type']\n",
        "best_score_chs = agglomerative_cluster_hyperparameter_tuning_df.sort_values(by= 'Calinski-Harabasz Score', ascending = False).iloc[0]['type']\n",
        "best_score_dbs = agglomerative_cluster_hyperparameter_tuning_df.sort_values(by= 'David-Bouldin Score', ascending = True).iloc[0]['type']\n",
        "\n",
        "print(f\"Parameters that gave the best clustering metrics: \\\n",
        "      \\nFor Silhouette Score : {(best_score_ss)} \\\n",
        "      \\nFor Calinski-Harabasz Score : {(best_score_chs)} \\\n",
        "      \\nFor David-Bouldin Score : {(best_score_dbs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will compare this with agglomerative clustering using the PCA data"
      ],
      "metadata": {
        "id": "-2ogvBAtzWV9"
      },
      "id": "-2ogvBAtzWV9"
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dataframe for storing scores and parameters used.\n",
        "pca_agglomerative_cluster_hyperparameter_tuning_df = pd.DataFrame({'affinity': [],\n",
        "                                                             'linkage': [],\n",
        "                                                             'Homogeneity Score': [],\n",
        "                                                             'Completeness Score': [],\n",
        "                                                             'V_measure Score': [],\n",
        "                                                             'Accuracy Score': [],\n",
        "                                                             'Balanced Accuracy Score': [],\n",
        "                                                             'F1 Score': [],\n",
        "                                                             'Adjusted_rand_score': [],\n",
        "                                                             'Silhouette Score': [],\n",
        "                                                             'Calinski-Harabasz Score': [],\n",
        "                                                             'David-Bouldin Score': []})\n",
        "for affinity in ['euclidean', 'l1', 'l2', 'manhattan', 'cosine']:\n",
        "    for linkage in ['ward', 'average', 'complete', 'single']:\n",
        "        #ward only works with euclidean so pass on all non euclidean affinities\n",
        "        if (linkage == 'ward') & np.any([affinity == 'l1' or\n",
        "                                      affinity == 'l2' or\n",
        "                                      affinity == 'manhattan' or\n",
        "                                      affinity == 'cosine']):\n",
        "            pass\n",
        "        else:\n",
        "            pca_agglomerative_cluster_model= AgglomerativeClustering(n_clusters = 2,\n",
        "                                                       affinity = affinity,\n",
        "                                                       linkage = linkage)\n",
        "            pca_agglomerative_cluster_model.fit(X_pca[:,:6])\n",
        "            h,c,v,a,b,f1,ars = classification_using_clustering_performance_evaluation(y_all, pca_agglomerative_cluster_model.labels_ , classification= False)\n",
        "            ss, chs, dbs =  clustering_performance_evaluation(X_all, pca_agglomerative_cluster_model.labels_)\n",
        "            pca_agglomerative_cluster_hyperparameter_tuning_df = \\\n",
        "                pca_agglomerative_cluster_hyperparameter_tuning_df.append({'affinity': affinity,\n",
        "                                                                     'linkage': linkage,\n",
        "                                                                     'Homogeneity Score': h,\n",
        "                                                                     'Completeness Score': c,\n",
        "                                                                     'V_measure Score': v,\n",
        "                                                                     'Accuracy Score': a,\n",
        "                                                                     'Balanced Accuracy Score': b,\n",
        "                                                                     'F1 Score': f1,\n",
        "                                                                     'Adjusted_rand_score': ars,\n",
        "                                                                     'Silhouette Score': ss,\n",
        "                                                                     'Calinski-Harabasz Score': chs,\n",
        "                                                                     'David-Bouldin Score': dbs}, ignore_index = True)                                                             "
      ],
      "metadata": {
        "id": "S_A6t7tl5PyR"
      },
      "id": "S_A6t7tl5PyR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_agglomerative_cluster_hyperparameter_tuning_df = pca_agglomerative_cluster_hyperparameter_tuning_df.sort_values(by= 'Adjusted_rand_score', ascending = False)\n",
        "pca_agglomerative_cluster_hyperparameter_tuning_df"
      ],
      "metadata": {
        "id": "AY1pcqt55ath"
      },
      "id": "AY1pcqt55ath",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_agglomerative_cluster_hyperparameter_tuning_df['type'] = pca_agglomerative_cluster_hyperparameter_tuning_df[['affinity','linkage']].apply(' '.join, axis = 1)"
      ],
      "metadata": {
        "id": "o5qJn5dG5d7J"
      },
      "id": "o5qJn5dG5d7J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the various scores\n",
        "fig = plt.figure(figsize = (10,40))\n",
        "for i,j in zip(range(1,11), ['Homogeneity Score',\n",
        "       'Completeness Score', 'V_measure Score', 'Accuracy Score',\n",
        "       'Balanced Accuracy Score', 'F1 Score', 'Adjusted_rand_score',\n",
        "       'Silhouette Score', 'Calinski-Harabasz Score', 'David-Bouldin Score']):\n",
        "    fig.add_subplot(5,2,i)\n",
        "    plt.plot(pca_agglomerative_cluster_hyperparameter_tuning_df['type'],\n",
        "            pca_agglomerative_cluster_hyperparameter_tuning_df[j])\n",
        "    plt.title(f\"{j}\\n Vs Agglomerative Hyperparameters\")\n",
        "    plt.xlabel(f\"Hyperparameters\")\n",
        "    plt.xticks(rotation = 90)\n",
        "    plt.ylabel(f\"{j}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eCDhNvHO5mZA"
      },
      "id": "eCDhNvHO5mZA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best Classification Parameters overall is for {pca_agglomerative_cluster_hyperparameter_tuning_df['type'].iloc[0]}\")"
      ],
      "metadata": {
        "id": "9loiYYvE5mRB"
      },
      "id": "9loiYYvE5mRB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_score_ss = pca_agglomerative_cluster_hyperparameter_tuning_df.sort_values(by= 'Silhouette Score', ascending = False).iloc[0]['type']\n",
        "best_score_chs = pca_agglomerative_cluster_hyperparameter_tuning_df.sort_values(by= 'Calinski-Harabasz Score', ascending = False).iloc[0]['type']\n",
        "best_score_dbs = pca_agglomerative_cluster_hyperparameter_tuning_df.sort_values(by= 'David-Bouldin Score', ascending = True).iloc[0]['type']\n",
        "\n",
        "print(f\"Parameters that gave the best clustering metrics: \\\n",
        "      \\nFor Silhouette Score : {(best_score_ss)} \\\n",
        "      \\nFor Calinski-Harabasz Score : {(best_score_chs)} \\\n",
        "      \\nFor David-Bouldin Score : {(best_score_dbs)}\")"
      ],
      "metadata": {
        "id": "pxamJWe96Ja5"
      },
      "id": "pxamJWe96Ja5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing the Agglomerative Clustering and PCA- Agglomerative Clustering:"
      ],
      "metadata": {
        "id": "NG35CQegEBPO"
      },
      "id": "NG35CQegEBPO"
    },
    {
      "cell_type": "code",
      "source": [
        "agglomerative_comparisons = pd.concat([pd.DataFrame(agglomerative_cluster_hyperparameter_tuning_df.sort_values(by = 'Adjusted_rand_score',ascending = False).iloc[0]),\n",
        "          pd.DataFrame(pca_agglomerative_cluster_hyperparameter_tuning_df.sort_values(by = 'Adjusted_rand_score',ascending = False).iloc[0])], axis = 1)\n",
        "agglomerative_comparisons.columns = ['Agglomerative Results', 'PCA Agglomerative Results'] \n",
        "agglomerative_comparisons"
      ],
      "metadata": {
        "id": "zrWpQoc8Du6P"
      },
      "id": "zrWpQoc8Du6P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA is giving worse scores overall"
      ],
      "metadata": {
        "id": "m5VCSDk_ERAv"
      },
      "id": "m5VCSDk_ERAv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gaussian Mixture Model\n",
        "\n",
        "GMM assume that all data points are genrated from a mixture of certain number of Gaussian distribution's, and each of these represents a cluster. "
      ],
      "metadata": {
        "id": "seNP7Y5LDoG9"
      },
      "id": "seNP7Y5LDoG9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Scaler\n",
        "# Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance)\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "#X_scaled = StandardScaler().fit_transform(X)"
      ],
      "metadata": {
        "id": "woaDh76kDorT"
      },
      "id": "woaDh76kDorT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling with Scaled data (Without PCA)"
      ],
      "metadata": {
        "id": "KZIE4YXq4fd1"
      },
      "id": "KZIE4YXq4fd1"
    },
    {
      "cell_type": "code",
      "source": [
        "## Gausian Mixture\n",
        "gm = GaussianMixture(n_components=2, covariance_type=\"full\")\n",
        "gm.fit(X_all)"
      ],
      "metadata": {
        "id": "X_Gx1DWiDqA9"
      },
      "id": "X_Gx1DWiDqA9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Weights are: {gm.weights_}\")\n",
        "print(f\"Converged status is: {gm.converged_}\")\n",
        "print(f\"Number of iterations needed for converging: {gm.n_iter_}\")\n",
        "#print(f\"Means with and without PCA and Scaling are {gm.means_} and {gm_scaled_pca.means_} respectively\")\n",
        "#print(f\"Covariances with and without PCA and Scaling are {gm.covariances_} and {gm_scaled_pca.covariances_} respectively\")"
      ],
      "metadata": {
        "id": "5kcn-UiODp4A"
      },
      "id": "5kcn-UiODp4A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm_pred = gm.predict(X_all)"
      ],
      "metadata": {
        "id": "v1WMNmUPDp1J"
      },
      "id": "v1WMNmUPDp1J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm.predict_proba(X_all)"
      ],
      "metadata": {
        "id": "ccICJhp6DpyD"
      },
      "id": "ccICJhp6DpyD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Evaluate the clustering methods using appropriate metrics such as the Adjusted Rand index, Homogeneity, Completeness and V-Measure, \n",
        "#    using the ground truth.\n",
        "# Evaluation Techniques --> https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules\n",
        "# Adjusted Rand index, Homogeneity, Completeness and V-Measure, using the ground truth.\n",
        "\n",
        "clustering_performance_evaluation(X_all, gm_pred, print_output = True)\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "classification_using_clustering_performance_evaluation(y_all, gm_pred, print_output = True, classification= False)"
      ],
      "metadata": {
        "id": "9KFXLJUqDpsD"
      },
      "id": "9KFXLJUqDpsD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyper Parameter tuning with Scaled data (Without PCA)"
      ],
      "metadata": {
        "id": "baDr5ZrH4Z2p"
      },
      "id": "baDr5ZrH4Z2p"
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dataframe for storing scores and parameters for Gaussian Mixture Model (https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)\n",
        "gmm_clustering_hyperparameter_tuning_df = pd.DataFrame({'covariance_type': [],\n",
        "                                                             'init_params': [],\n",
        "                                                             'Homogeneity Score': [],\n",
        "                                                             'Completeness Score': [],\n",
        "                                                             'V_measure Score': [],\n",
        "                                                             'Accuracy Score': [],\n",
        "                                                             'Balanced Accuracy Score': [],\n",
        "                                                             'F1 Score': [],\n",
        "                                                             'Adjusted_rand_score': [],\n",
        "                                                             'Silhouette Score': [],\n",
        "                                                             'Calinski-Harabasz Score': [],\n",
        "                                                             'David-Bouldin Score': []})"
      ],
      "metadata": {
        "id": "DxGmCaPZDpl8"
      },
      "id": "DxGmCaPZDpl8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for covariance_type in ['full','tied','diag', 'spherical']:\n",
        "  for param in ['kmeans','random']:\n",
        "    gmm_model = GaussianMixture(n_components=2, \n",
        "                                covariance_type=covariance_type,\n",
        "                                init_params=param\n",
        "                                )\n",
        "    gmm_model.fit(X_all)\n",
        "    gmm_pred = gmm_model.predict(X_all)\n",
        "    h,c,v,a,b,f1,ars = classification_using_clustering_performance_evaluation(y_all, gmm_pred, classification= True)\n",
        "    \n",
        "    ss, chs, dbs =  clustering_performance_evaluation(X_all, gmm_pred)\n",
        "\n",
        "    gmm_clustering_hyperparameter_tuning_df = \\\n",
        "                gmm_clustering_hyperparameter_tuning_df.append({'covariance_type': covariance_type,\n",
        "                                                                     'init_params': param,\n",
        "                                                                     'Homogeneity Score': h,\n",
        "                                                                     'Completeness Score': c,\n",
        "                                                                     'V_measure Score': v,\n",
        "                                                                     'Accuracy Score': a,\n",
        "                                                                     'Balanced Accuracy Score': b,\n",
        "                                                                     'F1 Score': f1,\n",
        "                                                                     'Adjusted_rand_score': ars,\n",
        "                                                                     'Silhouette Score': ss,\n",
        "                                                                     'Calinski-Harabasz Score': chs,\n",
        "                                                                     'David-Bouldin Score': dbs}, ignore_index = True)  \n"
      ],
      "metadata": {
        "id": "DgFdPhlWDpjB"
      },
      "id": "DgFdPhlWDpjB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gmm_clustering_hyperparameter_tuning_df = gmm_clustering_hyperparameter_tuning_df.sort_values(by= 'Adjusted_rand_score', ascending = False)\n",
        "gmm_clustering_hyperparameter_tuning_df"
      ],
      "metadata": {
        "id": "GyMsP18XDpgL"
      },
      "id": "GyMsP18XDpgL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling with PCA data"
      ],
      "metadata": {
        "id": "PjZZUt3B3CEF"
      },
      "id": "PjZZUt3B3CEF"
    },
    {
      "cell_type": "code",
      "source": [
        "gm_scaled_pca = GaussianMixture(n_components=2, covariance_type=\"full\")\n",
        "gm_scaled_pca.fit(X_pca)\n",
        "gm_pred_scaled_pca = gm_scaled_pca.predict(X_pca)"
      ],
      "metadata": {
        "id": "p0sLQpZz3cv1"
      },
      "id": "p0sLQpZz3cv1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm_scaled_pca.predict_proba(X_pca)"
      ],
      "metadata": {
        "id": "51O3fNEX3z6y"
      },
      "id": "51O3fNEX3z6y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Weights are: {gm_scaled_pca.weights_}\")\n",
        "print(f\"Converged status is: {gm_scaled_pca.converged_}\")\n",
        "print(f\"Number of iterations needed for converging: {gm_scaled_pca.n_iter_}\")"
      ],
      "metadata": {
        "id": "9-wc1aUk3oXf"
      },
      "id": "9-wc1aUk3oXf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Converged: \",gm_scaled_pca.converged_)\n",
        "print(\"Number of Iterations: \",gm_scaled_pca.n_iter_)\n",
        "print(\"Number of Features: \",gm_scaled_pca.n_features_in_)\n",
        "print(f\"Means with shape: {gm_scaled_pca.means_.shape} and values: {gm_scaled_pca.means_}\")\n",
        "print(f\"Weight with shape: {gm_scaled_pca.weights_.shape} and values: {gm_scaled_pca.weights_}\")"
      ],
      "metadata": {
        "id": "LT34MsAW3rop"
      },
      "id": "LT34MsAW3rop",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nScores on scaled and PCA data\")\n",
        "clustering_performance_evaluation(X_pca, gm_pred_scaled_pca, print_output = True)\n",
        "\n",
        "print(\"\\nScores on scaled and PCA data\")\n",
        "classification_using_clustering_performance_evaluation(y_all, gm_pred_scaled_pca, print_output = True, classification= False)"
      ],
      "metadata": {
        "id": "9EGl6zOA40sD"
      },
      "id": "9EGl6zOA40sD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyper Parameter tuning with PCA *data*"
      ],
      "metadata": {
        "id": "XxJwgfI2FGwS"
      },
      "id": "XxJwgfI2FGwS"
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dataframe for storing scores and parameters for Gaussian Mixture Model (https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)\n",
        "gmm_clustering_hyperparameter_tuning_pca_df = pd.DataFrame({'covariance_type': [],\n",
        "                                                             'init_params': [],\n",
        "                                                             'Homogeneity Score': [],\n",
        "                                                             'Completeness Score': [],\n",
        "                                                             'V_measure Score': [],\n",
        "                                                             'Accuracy Score': [],\n",
        "                                                             'Balanced Accuracy Score': [],\n",
        "                                                             'F1 Score': [],\n",
        "                                                             'Adjusted_rand_score': [],\n",
        "                                                             'Silhouette Score': [],\n",
        "                                                             'Calinski-Harabasz Score': [],\n",
        "                                                             'David-Bouldin Score': []})"
      ],
      "metadata": {
        "id": "gMvIuG67DpdN"
      },
      "id": "gMvIuG67DpdN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for covariance_type in ['full','tied','diag', 'spherical']:\n",
        "  for param in ['kmeans','random']:\n",
        "    gmm_model = GaussianMixture(n_components=2, \n",
        "                                covariance_type=covariance_type,\n",
        "                                init_params=param\n",
        "                                )\n",
        "    gmm_model.fit(X_pca)\n",
        "    gmm_pred = gmm_model.predict(X_pca)\n",
        "\n",
        "    h,c,v,a,b,f1,ars = classification_using_clustering_performance_evaluation(y_all, gmm_pred , classification= True)\n",
        "    \n",
        "    ss, chs, dbs =  clustering_performance_evaluation(X_pca, gmm_pred)\n",
        "\n",
        "    gmm_clustering_hyperparameter_tuning_pca_df = \\\n",
        "                gmm_clustering_hyperparameter_tuning_pca_df.append({'covariance_type': covariance_type,\n",
        "                                                                     'init_params': param,\n",
        "                                                                     'Homogeneity Score': h,\n",
        "                                                                     'Completeness Score': c,\n",
        "                                                                     'V_measure Score': v,\n",
        "                                                                     'Accuracy Score': a,\n",
        "                                                                     'Balanced Accuracy Score': b,\n",
        "                                                                     'F1 Score': f1,\n",
        "                                                                     'Adjusted_rand_score': ars,\n",
        "                                                                     'Silhouette Score': ss,\n",
        "                                                                     'Calinski-Harabasz Score': chs,\n",
        "                                                                     'David-Bouldin Score': dbs}, ignore_index = True)  \n"
      ],
      "metadata": {
        "id": "dVEUuAsSDpae"
      },
      "id": "dVEUuAsSDpae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gmm_clustering_hyperparameter_tuning_pca_df = gmm_clustering_hyperparameter_tuning_pca_df.sort_values(by= 'Adjusted_rand_score', ascending = False)\n",
        "gmm_clustering_hyperparameter_tuning_pca_df"
      ],
      "metadata": {
        "id": "9eqmxmUMDpXi"
      },
      "id": "9eqmxmUMDpXi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plots\n",
        "fig, ((ax1, ax2, ax3)) = plt.subplots(1, 3, sharey=True, tight_layout=True, figsize=(20,10))\n",
        "fig.suptitle(\"Cluster Distribution\", fontsize=30, fontweight='bold', y=1.05) # y value to place sup title slightly outside the figure.\n",
        "\n",
        "ax1.scatter(X_all[:,0][y_all == 0], X_all[:,1][y_all == 0], label = 'Benign', alpha=0.45)\n",
        "ax1.scatter(X_all[:,0][y_all == 1], X_all[:,1][y_all == 1], label = 'Malignant', alpha=0.45)\n",
        "ax1.set_title(\"Actual clusters\", fontsize=15, fontweight='bold')\n",
        "ax1.set_xticks([])\n",
        "ax1.set_yticks([])\n",
        "ax1.set_xlabel(X_columns[0])\n",
        "ax1.set_ylabel(X_columns[1])\n",
        "ax1.legend()\n",
        "\n",
        "ax2.scatter(X_all[:,0][gm_pred == 1], X_all[:,1][gm_pred == 1], label = 'Benign', alpha=0.45)\n",
        "ax2.scatter(X_all[:,0][gm_pred == 0], X_all[:,1][gm_pred == 0], label = 'Malignant', alpha=0.45)\n",
        "ax2.set_title(\"Modeled Clusters on Scaled Data\", fontsize=15, fontweight='bold')\n",
        "ax2.set_xticks([])\n",
        "ax2.set_yticks([])\n",
        "ax2.set_xlabel(X_columns[0])\n",
        "ax2.legend()\n",
        "\n",
        "ax3.scatter(X_pca[:,0][gm_pred_scaled_pca == 0], X_pca[:,1][gm_pred_scaled_pca == 0], label = 'Benign', alpha=0.45)\n",
        "ax3.scatter(X_pca[:,0][gm_pred_scaled_pca == 1], X_pca[:,1][gm_pred_scaled_pca == 1], label = 'Malignant', alpha=0.45)\n",
        "ax3.set_title(\"Modeled Clusters on PCA data\", fontsize=15, fontweight='bold')\n",
        "ax3.set_xticks([])\n",
        "ax3.set_yticks([])\n",
        "ax3.set_xlabel(X_columns[0])\n",
        "ax3.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "zp-GDXoJ4Mvv"
      },
      "id": "zp-GDXoJ4Mvv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean Shift\n",
        "\n",
        "Mean shift clustering is a hierarchial clustering algorithm which aims to discover blobs in a smooth density of samples.It works by updating candidate for centroids to be the mean of the point within a certain region.\n"
      ],
      "metadata": {
        "id": "K5uOB3ddFPCh"
      },
      "id": "K5uOB3ddFPCh"
    },
    {
      "cell_type": "code",
      "source": [
        "bandwidth = estimate_bandwidth(X_all, quantile=0.2, n_samples=500)\n",
        "bandwidth"
      ],
      "metadata": {
        "id": "adaOJGX7DpUE"
      },
      "id": "adaOJGX7DpUE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling with Scaled data"
      ],
      "metadata": {
        "id": "oJAvARrcweG-"
      },
      "id": "oJAvARrcweG-"
    },
    {
      "cell_type": "code",
      "source": [
        "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
        "ms.fit(X_all)\n",
        "ms_pred = ms.predict(X_all)"
      ],
      "metadata": {
        "id": "2zJla_CqDpQr"
      },
      "id": "2zJla_CqDpQr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(ms_pred)"
      ],
      "metadata": {
        "id": "Jw_qBTa1F7RS"
      },
      "id": "Jw_qBTa1F7RS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling with PCA data"
      ],
      "metadata": {
        "id": "TSlSYZyywMv0"
      },
      "id": "TSlSYZyywMv0"
    },
    {
      "cell_type": "code",
      "source": [
        "ms_scaled_pca = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
        "ms_scaled_pca.fit(X_pca)\n",
        "ms_pred_scaled_pca = ms_scaled_pca.predict(X_pca)"
      ],
      "metadata": {
        "id": "OfOkV-kOwNa7"
      },
      "id": "OfOkV-kOwNa7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(ms_pred_scaled_pca)"
      ],
      "metadata": {
        "id": "TWuMUIR6xF43"
      },
      "id": "TWuMUIR6xF43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plots\n",
        "fig, ((ax1, ax2, ax3)) = plt.subplots(1, 3, sharey=True, tight_layout=True, figsize=(20,10))\n",
        "fig.suptitle(\"Cluster Distribution\", fontsize=30, fontweight='bold', y=1.05) # y value to place sup title slightly outside the figure.\n",
        "\n",
        "ax1.scatter(X_all[:,0][y_all == 0], X_all[:,1][y_all == 0], label = 'Benign', alpha=0.45)\n",
        "ax1.scatter(X_all[:,0][y_all == 1], X_all[:,1][y_all == 1], label = 'Malignant', alpha=0.45)\n",
        "ax1.set_title(\"Actual clusters\", fontsize=15, fontweight='bold')\n",
        "ax1.set_xticks([])\n",
        "ax1.set_yticks([])\n",
        "ax1.set_xlabel(X_columns[0])\n",
        "ax1.set_ylabel(X_columns[1])\n",
        "ax1.legend()\n",
        "\n",
        "ax2.scatter(X_all[:,0][ms_pred == 1], X_all[:,1][ms_pred == 1], label = 'Benign', alpha=0.45)\n",
        "ax2.scatter(X_all[:,0][ms_pred == 0], X_all[:,1][ms_pred == 0], label = 'Malignant', alpha=0.45)\n",
        "ax2.set_title(\"Modeled Clusters on Scaled Data\", fontsize=15, fontweight='bold')\n",
        "ax2.set_xticks([])\n",
        "ax2.set_yticks([])\n",
        "ax2.set_xlabel(X_columns[0])\n",
        "ax2.legend()\n",
        "\n",
        "ax3.scatter(X_pca[:,0][ms_pred_scaled_pca == 1], X_pca[:,1][ms_pred_scaled_pca == 1], label = 'Benign', alpha=0.45)\n",
        "ax3.scatter(X_pca[:,0][ms_pred_scaled_pca == 0], X_pca[:,1][ms_pred_scaled_pca == 0], label = 'Malignant', alpha=0.45)\n",
        "ax3.set_title(\"Modeled Clusters on PCA data\", fontsize=15, fontweight='bold')\n",
        "ax3.set_xticks([])\n",
        "ax3.set_yticks([])\n",
        "ax3.set_xlabel(X_columns[0])\n",
        "ax3.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Msy0fCS6F7Lm"
      },
      "id": "Msy0fCS6F7Lm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nScores on scaled and PCA data\")\n",
        "clustering_performance_evaluation(X_pca, ms_pred_scaled_pca, print_output = True)"
      ],
      "metadata": {
        "id": "yh0nHPsrF7I8"
      },
      "id": "yh0nHPsrF7I8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Scores on not scaled data\")\n",
        "classification_using_clustering_performance_evaluation(y_all, ms_pred, print_output = True, classification= False)"
      ],
      "metadata": {
        "id": "0oWvMHi_F7Cj"
      },
      "id": "0oWvMHi_F7Cj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dataframe for storing scores and parameters for Gaussian Mixture Model (https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)\n",
        "ms_clustering_hyperparameter_tuning_pca_df = pd.DataFrame({'covariance_type': [],\n",
        "                                                             'cluster_all': [],\n",
        "                                                             'Homogeneity Score': [],\n",
        "                                                             'Completeness Score': [],\n",
        "                                                             'V_measure Score': [],\n",
        "                                                             'Accuracy Score': [],\n",
        "                                                             'Balanced Accuracy Score': [],\n",
        "                                                             'F1 Score': [],\n",
        "                                                             'Adjusted_rand_score': [],\n",
        "                                                             'Silhouette Score': [],\n",
        "                                                             'Calinski-Harabasz Score': [],\n",
        "                                                             'David-Bouldin Score': []})"
      ],
      "metadata": {
        "id": "gx-hK3WzzePk"
      },
      "id": "gx-hK3WzzePk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On Upscaled data - no clustering\n",
        "ON PCA - 5 labels predicted. No way to spcify number of clusters. Bad choice for Predicting in this data, hence no hyper parameter tuning"
      ],
      "metadata": {
        "id": "vitQrP_c6BQQ"
      },
      "id": "vitQrP_c6BQQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Means Clustering\n",
        "\n",
        "Kmaens clustering is an iterative algorithm that tries to seprate the dataset in to *K* pre-define distinct non overlappong clusters. It tries to make the intracluster data points as similar as possible while also keeping the clusters as fas as possible.\n",
        "\n",
        "https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a"
      ],
      "metadata": {
        "id": "7AESd7vA_hrh"
      },
      "id": "7AESd7vA_hrh"
    },
    {
      "cell_type": "code",
      "source": [
        "#Standard scalar to perform scaling of features\n",
        "#X = StandardScaler().fit_transform(X_all)\n",
        "\n",
        "#K-Means Unsupervisied Machine Learning\n",
        "kmeans = KMeans(n_clusters = 2, init='k-means++', max_iter=300, random_state = None, n_init = 10)\n",
        "kmeans_model = kmeans.fit(X_all)\n",
        "kmeans_model_predict = kmeans.predict(X_all)\n",
        "kmeans_predictions = kmeans.labels_\n",
        "kmeans_predictions\n",
        "print(kmeans.get_params)"
      ],
      "metadata": {
        "id": "rAESbg5AATrw"
      },
      "id": "rAESbg5AATrw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans_predictions"
      ],
      "metadata": {
        "id": "gypOwu8Pjqhf"
      },
      "id": "gypOwu8Pjqhf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating model performance on clustering metrics\n",
        "clustering_performance_evaluation(X_all, kmeans_model.labels_, print_output = True)"
      ],
      "metadata": {
        "id": "wYZVdwPjAToY"
      },
      "id": "wYZVdwPjAToY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating model performance on classification metrics \n",
        "value = classification_using_clustering_performance_evaluation(y_all, kmeans_model.labels_, print_output= True, classification= False)"
      ],
      "metadata": {
        "id": "tqV3-f6MATkK"
      },
      "id": "tqV3-f6MATkK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter tuning for K-Means:\n",
        "\n",
        "#Creating Dataframe to store score\n",
        "\n",
        "kmeans_cluster_hyperparameter_tuning_df = pd.DataFrame({'n_init': [],\n",
        "                                                             'init' :[],\n",
        "                                                             'algorithm' :[],\n",
        "                                                             'Homogeneity Score': [],\n",
        "                                                             'Completeness Score': [],\n",
        "                                                             'V_measure Score': [],\n",
        "                                                             'Accuracy Score': [],\n",
        "                                                             'Balanced Accuracy Score': [],\n",
        "                                                             'F1 Score': [],\n",
        "                                                             'Adjusted_rand_score': [],\n",
        "                                                             'Silhouette Score': [],\n",
        "                                                             'Calinski-Harabasz Score': [],\n",
        "                                                             'David-Bouldin Score': []})"
      ],
      "metadata": {
        "id": "nSDpTXXqAThF"
      },
      "id": "nSDpTXXqAThF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n_init in [10,100,1000]:\n",
        "  for init in ['k-means++', 'random']:\n",
        "    for algorithm in ['auto', 'full','elkan']:\n",
        "      kmeans_cluster_model = KMeans(n_clusters=2,\n",
        "                                    max_iter=300,\n",
        "                                    random_state= 1,\n",
        "                                    n_init = n_init,\n",
        "                                    init = init,\n",
        "                                    algorithm = algorithm)\n",
        "      kmeans_cluster_model.fit(X_all)\n",
        "      h,c,v,a,b,f1,ars = classification_using_clustering_performance_evaluation(y_all, kmeans_cluster_model.labels_, classification = False)\n",
        "      ss, chs,dbs = clustering_performance_evaluation(X_all, kmeans_cluster_model.labels_)\n",
        "      kmeans_cluster_hyperparameter_tuning_df = \\\n",
        "        kmeans_cluster_hyperparameter_tuning_df.append({'n_init': n_init,\n",
        "                                                        'init' : init,\n",
        "                                                        'algorithm' : algorithm,\n",
        "                                                        'Homogeneity Score': h,\n",
        "                                                        'Completeness Score': c,\n",
        "                                                        'V_measure Score': v,\n",
        "                                                        'Accuracy Score': a,\n",
        "                                                        'Balanced Accuracy Score': b,\n",
        "                                                        'F1 Score': f1,\n",
        "                                                        'Adjusted_rand_score': ars,\n",
        "                                                        'Silhouette Score': ss,\n",
        "                                                        'Calinski-Harabasz Score': chs,\n",
        "                                                        'David-Bouldin Score': dbs}, ignore_index = True)"
      ],
      "metadata": {
        "id": "rO0je0l0ATeS"
      },
      "id": "rO0je0l0ATeS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing scores and parameters in descending order\n",
        "\n",
        "kmeans_cluster_hyperparameter_tuning_df.sort_values(by = 'Adjusted_rand_score', ascending = False)"
      ],
      "metadata": {
        "id": "ihylam9XATbE"
      },
      "id": "ihylam9XATbE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Principal Component Analysis (Linear dimensionality reduction)\n",
        "\n",
        "#pca = PCA(n_components=2)\n",
        "#pca.fit(X)\n",
        "\n",
        "#x_pca = pca.transform(X)\n",
        "\n",
        "kmeans_pca = KMeans(n_clusters = 2, init='k-means++', max_iter=300, random_state = 1, n_init = 10)\n",
        "kmeans_pca_model = kmeans.fit(X_pca)\n",
        "kmeans_pca_model_predict = kmeans.predict(X_pca)\n",
        "kmeans_pca_predictions = kmeans.labels_\n",
        "kmeans_pca_predictions\n",
        "print(kmeans_pca.get_params)\n",
        "\n"
      ],
      "metadata": {
        "id": "A0CyWVhwATW6"
      },
      "id": "A0CyWVhwATW6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating model performance on clustering metrics\n",
        "clustering_performance_evaluation(X_pca, kmeans_pca_model.labels_, print_output = True)"
      ],
      "metadata": {
        "id": "ZpWGXf4tATUS"
      },
      "id": "ZpWGXf4tATUS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Evaluating model performance on classification metrics\n",
        "value = classification_using_clustering_performance_evaluation(y_all, kmeans_pca_model.labels_, print_output= True, classification= False)"
      ],
      "metadata": {
        "id": "v2qcX1Qu_ioh"
      },
      "id": "v2qcX1Qu_ioh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter tuning for PCA K-Means:\n",
        "\n",
        "#Creating Dataframe to store score\n",
        "\n",
        "kmeans_pca_cluster_hyperparameter_tuning_df = pd.DataFrame({'n_init': [],\n",
        "                                                             'init' :[],\n",
        "                                                             'algorithm' :[],\n",
        "                                                             'Homogeneity Score': [],\n",
        "                                                             'Completeness Score': [],\n",
        "                                                             'V_measure Score': [],\n",
        "                                                             'Accuracy Score': [],\n",
        "                                                             'Balanced Accuracy Score': [],\n",
        "                                                             'F1 Score': [],\n",
        "                                                             'Adjusted_rand_score': [],\n",
        "                                                             'Silhouette Score': [],\n",
        "                                                             'Calinski-Harabasz Score': [],\n",
        "                                                             'David-Bouldin Score': []})"
      ],
      "metadata": {
        "id": "-Sdf41dtCkrR"
      },
      "id": "-Sdf41dtCkrR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n_init in [10,100,1000]:\n",
        "  for init in ['k-means++', 'random']:\n",
        "    for algorithm in ['auto', 'full','elkan']:\n",
        "      kmeans_pca_cluster_model = KMeans(n_clusters=2,\n",
        "                                    max_iter=300,\n",
        "                                    random_state= 1,\n",
        "                                    n_init = n_init,\n",
        "                                    init = init,\n",
        "                                    algorithm = algorithm)\n",
        "      kmeans_pca_cluster_model.fit(X_pca)\n",
        "      h,c,v,a,b,f1,ars = classification_using_clustering_performance_evaluation(y_all, kmeans_pca_cluster_model.labels_, classification = False)\n",
        "      ss, chs,dbs = clustering_performance_evaluation(X_pca, kmeans_pca_cluster_model.labels_)\n",
        "      kmeans_pca_cluster_hyperparameter_tuning_df = \\\n",
        "        kmeans_pca_cluster_hyperparameter_tuning_df.append({'n_init': n_init,\n",
        "                                                        'init' : init,\n",
        "                                                        'algorithm' : algorithm,\n",
        "                                                        'Homogeneity Score': h,\n",
        "                                                        'Completeness Score': c,\n",
        "                                                        'V_measure Score': v,\n",
        "                                                        'Accuracy Score': a,\n",
        "                                                        'Balanced Accuracy Score': b,\n",
        "                                                        'F1 Score': f1,\n",
        "                                                        'Adjusted_rand_score': ars,\n",
        "                                                        'Silhouette Score': ss,\n",
        "                                                        'Calinski-Harabasz Score': chs,\n",
        "                                                        'David-Bouldin Score': dbs}, ignore_index = True)"
      ],
      "metadata": {
        "id": "ZYOAXR7ZCknt"
      },
      "id": "ZYOAXR7ZCknt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans_pca_cluster_hyperparameter_tuning_df.sort_values(by = 'Adjusted_rand_score', ascending = False)"
      ],
      "metadata": {
        "id": "Ion5pBB7CkkN"
      },
      "id": "Ion5pBB7CkkN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kernel PCA\n",
        "\n",
        "kpca = KernelPCA(n_components=2, kernel='linear')\n",
        "#kpca = KernelPCA(n_components=2, kernel='cosine')\n",
        "#kpca = KernelPCA(n_components=2, kernel='rbf')\n",
        "kpca.fit(X_all)\n",
        "\n",
        "X_kpca = kpca.transform(X_all)\n",
        "\n",
        "kmeans_kpca = KMeans(n_clusters = 2, init='k-means++', max_iter=300, random_state = None, n_init = 10)\n",
        "kmeans_kpca_model = kmeans.fit(X_kpca)\n",
        "kmeans_kpca_model_predict = kmeans.predict(X_kpca)\n",
        "kmeans_kpca_predictions = kmeans.labels_\n",
        "kmeans_kpca_predictions\n",
        "print(kmeans_kpca.get_params)\n",
        "\n"
      ],
      "metadata": {
        "id": "yxKPs2lxCkhO"
      },
      "id": "yxKPs2lxCkhO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_performance_evaluation(X_kpca, kmeans_kpca_model.labels_, print_output = True)"
      ],
      "metadata": {
        "id": "wqnGP7v6Ckdn"
      },
      "id": "wqnGP7v6Ckdn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plots\n",
        "fig, ((ax1, ax2, ax3, ax4)) = plt.subplots(1, 4, sharey=True, tight_layout=True, figsize=(20,10))\n",
        "fig.suptitle(\"Cluster Distribution\", fontsize=30, fontweight='bold', y=1.05) # y value to place sup title slightly outside the figure.\n",
        "\n",
        "ax1.scatter(X_all[:,0][y_all == 0], X_all[:,1][y_all == 0], label = 'Benign', alpha=0.45)\n",
        "ax1.scatter(X_all[:,0][y_all == 1], X_all[:,1][y_all == 1], label = 'Malignant', alpha=0.45)\n",
        "ax1.set_title(\"Actual clusters\", fontsize=15, fontweight='bold')\n",
        "ax1.set_xticks([])\n",
        "ax1.set_yticks([])\n",
        "ax1.set_xlabel(X_columns[0])\n",
        "ax1.set_ylabel(X_columns[1])\n",
        "ax1.legend()\n",
        "\n",
        "ax2.scatter(X_all[:,0][kmeans_model_predict == 1], X_all[:,1][kmeans_model_predict == 1], label = 'Benign', alpha=0.45)\n",
        "ax2.scatter(X_all[:,0][kmeans_model_predict == 0], X_all[:,1][kmeans_model_predict == 0], label = 'Malignant', alpha=0.45)\n",
        "ax2.set_title(\"Modeled Clusters on Scaled Data\", fontsize=15, fontweight='bold')\n",
        "ax2.set_xticks([])\n",
        "ax2.set_yticks([])\n",
        "ax2.set_xlabel(X_columns[0])\n",
        "ax2.legend()\n",
        "\n",
        "ax3.scatter(X_pca[:,0][kmeans_pca_model_predict == 1], X_pca[:,1][kmeans_pca_model_predict == 1], label = 'Benign', alpha=0.45)\n",
        "ax3.scatter(X_pca[:,0][kmeans_pca_model_predict == 0], X_pca[:,1][kmeans_pca_model_predict == 0], label = 'Malignant', alpha=0.45)\n",
        "ax3.set_title(\"Modeled Clusters on PCA data\", fontsize=15, fontweight='bold')\n",
        "ax3.set_xticks([])\n",
        "ax3.set_yticks([])\n",
        "ax3.set_xlabel(X_columns[0])\n",
        "ax3.legend()\n",
        "\n",
        "ax4.scatter(X_kpca[:,0][kmeans_kpca_model_predict == 1], X_kpca[:,1][kmeans_kpca_model_predict == 1], label = 'Benign', alpha=0.45)\n",
        "ax4.scatter(X_kpca[:,0][kmeans_kpca_model_predict == 0], X_kpca[:,1][kmeans_kpca_model_predict == 0], label = 'Malignant', alpha=0.45)\n",
        "ax4.set_title(\"Modeled Clusters on K-PCA data\", fontsize=15, fontweight='bold')\n",
        "ax4.set_xticks([])\n",
        "ax4.set_yticks([])\n",
        "ax4.set_xlabel(X_columns[0])\n",
        "ax4.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Uho2mJh8Gj7w"
      },
      "id": "Uho2mJh8Gj7w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DBSCAN Clustering\n",
        "\n",
        "DBSCAN is a clustering algorith that doesn't require the number of clusters to be predetermined as a hyperparameter. However, it does require epsilon and min_samples to be tuned for better clustering.\n",
        "\n",
        "The algorithm was implemented on the the PCA reduced dataset to determine if it provided a good clustering result.\n",
        "\n",
        "Strategy for  DBSCAN Clustering are as follows:\n",
        "1. Determine initial hyperparameters values for eps and min_samples.\n",
        "2. Fine Tune hyperparameters around identified values in step 1 for best clustering.\n",
        "3. Evaluate Clustering Performance\n",
        "\n"
      ],
      "metadata": {
        "id": "c_BjytylxGeZ"
      },
      "id": "c_BjytylxGeZ"
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing features in X dataframe and target label (y) as numpy array\n",
        "X_db = X.to_numpy().copy()\n",
        "y_db_old = y.to_numpy().copy()\n",
        "y_db = y_db_old.reshape((-1,1))\n",
        "\n",
        "#Concatenating both X_db And Y_db arrays for plotting purposes later\n",
        "x_db_concat = np.concatenate((X_db,y_db), axis=1)\n",
        "benign = x_db_concat[np.where(x_db_concat[:,-1]==1)]\n",
        "malignant = x_db_concat[np.where(x_db_concat[:,-1]==0)]\n",
        "\n",
        "#PCA reduction\n",
        "db_pca = PCA()\n",
        "db_x_red =db_pca.fit_transform(X_db)\n",
        "\n",
        "db_pca_var_perc = [100*i for i in db_pca.explained_variance_ratio_]"
      ],
      "metadata": {
        "id": "szvCpigoxeQC"
      },
      "id": "szvCpigoxeQC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Earlier, it was shown that the first 7 PCA components cumulatively explained 85% of the variance in the Data. With this information, PCA was conducted once again to extract only the first 7 PCA Components for clustering."
      ],
      "metadata": {
        "id": "YsvOollO33Fl"
      },
      "id": "YsvOollO33Fl"
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_db)"
      ],
      "metadata": {
        "id": "OXluQXx79r10"
      },
      "id": "OXluQXx79r10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the explained variance ratio as percentage\n",
        "\n",
        "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(10,10))\n",
        "ax.bar(x=[i for i in range(len(db_pca_var_perc))],height=db_pca_var_perc)\n",
        "ax.set_title('PCA components explanied variance in percentage')\n",
        "ax.set_xlabel('Principal Component Axis')\n",
        "ax.set_ylabel('Percentage of explained variance')\n",
        "\n",
        "db_pca_perc_sum = 0\n",
        "db_pca_count = 0\n",
        "\n",
        "for i in db_pca_var_perc:\n",
        "  if(db_pca_perc_sum < 85):\n",
        "    db_pca_perc_sum += i\n",
        "    db_pca_count += 1\n",
        "\n",
        "print(f'{db_pca_count} PCA components produces a cumulative variance of {round(db_pca_perc_sum,2)}% and can be used for further study')"
      ],
      "metadata": {
        "id": "F_91wkKbxeMi"
      },
      "id": "F_91wkKbxeMi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA To produce 7 components\n",
        "db_x_red = PCA(n_components=7).fit_transform(X_db)\n",
        "x_red_concat = np.concatenate((db_x_red,y_db), axis=1)\n",
        "\n",
        "benign = x_red_concat[np.where(x_red_concat[:,-1]==1)]\n",
        "malignant = x_red_concat[np.where(x_red_concat[:,-1]==0)]\n",
        "\n",
        "bening_1_pca_1 = benign[:,0]\n",
        "bening_1_pca_2 = benign[:,1]\n",
        "\n",
        "malignant_1_pca_1 = malignant[:,0]\n",
        "malignant_1_pca_2 = malignant[:,1]\n",
        "\n",
        "\n",
        "fig,ax = plt.subplots(nrows=1,ncols=1)\n",
        "ax.scatter(bening_1_pca_1,bening_1_pca_2,label = 'benign',alpha =0.1)\n",
        "ax.scatter(malignant_1_pca_1,malignant_1_pca_2,label = 'malignant',alpha=0.1)\n",
        "ax.set_xlabel('Principal component 1')\n",
        "ax.set_ylabel('Principal component 2')\n",
        "ax.legend()"
      ],
      "metadata": {
        "id": "yLWTS9YOxeJH"
      },
      "id": "yLWTS9YOxeJH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''General practise suggested is to use 2 times the dimension.Since we're using 7 PCA \n",
        "components we ill be using a value of 14  (Schubert et al., 2017)'''\n",
        "min_samples = 14\n",
        "\n",
        "\n",
        "'''As per Rahmah and Sitanggang, 2016 a solution to identify a good eps value \n",
        "is to run a nearest neighbour algorithm with the nearest neighbours being the \n",
        "same value as the min_samples chosen. Average distance between each sample and \n",
        "its identified neighbour is calculated, sorted and plotted. The elbow point in the\n",
        "plotted grap (point with the biggest curve) is chosen as the eps value'''\n",
        "\n",
        "knearest = NearestNeighbors(n_neighbors=min_samples).fit(db_x_red)\n",
        "cal_dist,ind = knearest.kneighbors(db_x_red)\n",
        "sort_dist = np.sort(cal_dist,axis=0)[:,1]\n",
        "\n",
        "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8))\n",
        "ax.plot(sort_dist)\n",
        "ax.set_xlabel('Data Point Index')\n",
        "ax.set_ylabel('Average Distance')\n",
        "ax.set_title('Curve to identify optimum eps value for DBSCAN')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "OfXXRVvWxeF8"
      },
      "id": "OfXXRVvWxeF8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The biggest curvature appears to fall within the range of 2 to 6. Parameter tuning will now be conducted in this range for the selected min_sample value of 12."
      ],
      "metadata": {
        "id": "5S0Z4PWL4AwN"
      },
      "id": "5S0Z4PWL4AwN"
    },
    {
      "cell_type": "code",
      "source": [
        "#choose what evaluation metric to use\n",
        "#create df\n",
        "#hyper parameter tuning\n",
        "#graph showing how parameter changes affect scores\n",
        "  \n",
        "\n",
        "#Evaluation\n",
        "parameter_score_dict = {'eps':[],'min_samples':[],'cluster_count':[],'homogeneity_score': [],\n",
        "'completeness_score':[],'v_measure_score': [],'adjusted_rand_score': [],\t\n",
        "'silhouette_score': [],'calinski-harabasz_score': [],\t'david-bouldin_score': []}\n",
        "\n",
        "for i in np.arange(2,6,0.1):\n",
        "  dbscan = cluster.DBSCAN(eps=i, min_samples=min_samples)\n",
        "  clust_labels = dbscan.fit_predict(db_x_red)\n",
        "  noise_count = list(clust_labels).count(-1)\n",
        "  \n",
        "  if noise_count > 0:\n",
        "    clust_count =  len(set(clust_labels)) - 1\n",
        "  else:\n",
        "    clust_count =  len(set(clust_labels))\n",
        "  \n",
        "  parameter_score_dict['eps'].append(i)\n",
        "  parameter_score_dict['min_samples'].append(min_samples)\n",
        "  parameter_score_dict['cluster_count'].append(clust_count)\n",
        "  parameter_score_dict['homogeneity_score'].append(homogeneity_score(y_db_old,clust_labels))\n",
        "  parameter_score_dict['completeness_score'].append(completeness_score(y_db_old,clust_labels))\n",
        "  parameter_score_dict['v_measure_score'].append(v_measure_score(y_db_old,clust_labels))\n",
        "  parameter_score_dict['adjusted_rand_score'].append(adjusted_rand_score(y_db_old,clust_labels))\n",
        "  parameter_score_dict['silhouette_score'].append(silhouette_score(db_x_red,clust_labels))\n",
        "  parameter_score_dict['calinski-harabasz_score'].append(calinski_harabasz_score(db_x_red,clust_labels))\n",
        "  parameter_score_dict['david-bouldin_score'].append(davies_bouldin_score(db_x_red,clust_labels))\n",
        "\n",
        "results = np.concatenate((np.array(parameter_score_dict['eps']).reshape((-1,1)),np.array(parameter_score_dict['min_samples']).reshape((-1,1)),\n",
        "                     np.array(parameter_score_dict['cluster_count']).reshape((-1,1)),\n",
        "                     np.array(parameter_score_dict['homogeneity_score']).reshape((-1,1)),\n",
        "                     np.array(parameter_score_dict['completeness_score']).reshape((-1,1)),\n",
        "                     np.array(parameter_score_dict['v_measure_score']).reshape((-1,1)),np.array(parameter_score_dict['adjusted_rand_score']).reshape((-1,1)),\n",
        "                     np.array(parameter_score_dict['silhouette_score']).reshape((-1,1)),np.array(parameter_score_dict['calinski-harabasz_score']).reshape((-1,1)),\n",
        "                     np.array(parameter_score_dict['david-bouldin_score']).reshape((-1,1))),axis=1)"
      ],
      "metadata": {
        "id": "VkEkBhMcxeDP"
      },
      "id": "VkEkBhMcxeDP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#results in numpy format\n",
        "\n",
        "'''\n",
        "Column information is as follows\n",
        "column 1 - eps\n",
        "column 2 - min_samples\n",
        "column 3 - cluster_count\n",
        "column 4 - homogeneity_score\n",
        "column 5 - completeness_score\n",
        "column 6 - v_measure_score\n",
        "column 7 - adjusted_rand_score\n",
        "column 8 - silhouette_score\n",
        "column 9 - calinski-harabasz_score\n",
        "column 10 - david-bouldin_score\n",
        "'''\n",
        "print(results)"
      ],
      "metadata": {
        "id": "w-yoZgpoxd_m"
      },
      "id": "w-yoZgpoxd_m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The same information presented using pandas for easy visualisation\n",
        "results_df =pd.DataFrame(parameter_score_dict)\n",
        "res_df_cols = results_df.columns\n",
        "results_df"
      ],
      "metadata": {
        "id": "T6RXYtb3xd8P"
      },
      "id": "T6RXYtb3xd8P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the purpose is to identify two clusters as per our target values the numpy array is filtered to identify results thats generated 2 clusters."
      ],
      "metadata": {
        "id": "Hdqa7JQE4WA1"
      },
      "id": "Hdqa7JQE4WA1"
    },
    {
      "cell_type": "code",
      "source": [
        "#filtering for two clusters\n",
        "two_cluster_results = results[results[:,2]==2]\n",
        "two_cluster_results\n",
        "filtered_results = results[np.where(results[:,2]==2)]"
      ],
      "metadata": {
        "id": "JUA9O_jexd4Z"
      },
      "id": "JUA9O_jexd4Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan_results = pd.DataFrame(filtered_results,columns=res_df_cols)\n",
        "dbscan_results"
      ],
      "metadata": {
        "id": "cESCOb23xdzl"
      },
      "id": "cESCOb23xdzl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eps_arr = two_cluster_results[:,0]\n",
        "min_samples_arr = two_cluster_results[:,1]\n",
        "two_cluster_results[:,1]\n",
        "\n",
        "best_sil_score = max(two_cluster_results[:,7])\n",
        "eps_sil_score = two_cluster_results[np.where(two_cluster_results[:,7]==best_sil_score)][0][0]\n",
        "best_ch_score = max(two_cluster_results[:,8])\n",
        "eps_ch_score = two_cluster_results[np.where(two_cluster_results[:,8]==best_ch_score)][0][0]\n",
        "best_db_score = max(two_cluster_results[:,9])\n",
        "eps_db_score = two_cluster_results[np.where(two_cluster_results[:,9]==best_db_score)][0][0]\n",
        "best_rand_score =  max(two_cluster_results[:,6])\n",
        "eps_rand_score = two_cluster_results[np.where(two_cluster_results[:,6]==best_rand_score)][0][0]"
      ],
      "metadata": {
        "id": "ShNgqRcWxdvP"
      },
      "id": "ShNgqRcWxdvP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The best parameters for the best identified silhouette score {round(best_sil_score,2)} are:  eps - {round(eps_sil_score,1)}  min_samples -{min_samples}')\n",
        "print(f'The best parameters for the best identified calinski-harabasz score {round(best_ch_score,2)} are:  eps - {round(eps_ch_score,1)}  min_samples -{min_samples}')\n",
        "print(f'The best parameters for the best identified david-bouldin score {round(best_db_score,2)} are:  eps - {round(eps_db_score,1)}  min_samples -{min_samples}')\n",
        "print(f'The best parameters for the best identified Adjusted Rand score {round(best_rand_score,2)} are:  eps - {round(eps_rand_score,1)}  min_samples -{min_samples}')"
      ],
      "metadata": {
        "id": "u4hqM8mexdpq"
      },
      "id": "u4hqM8mexdpq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = cluster.DBSCAN(eps=eps_sil_score, min_samples=min_samples)\n",
        "clust_labels = dbscan.fit_predict(db_x_red)\n",
        "\n",
        "new_concat = np.concatenate((db_x_red,np.array(clust_labels).reshape((-1,1))),axis=1)\n",
        "\n",
        "out_x1 = new_concat[new_concat[:,-1]==-1][:,0]\n",
        "out_x2 = new_concat[new_concat[:,-1]==-1][:,1]\n",
        "\n",
        "clust_1_x1  = new_concat[new_concat[:,-1]==0][:,0]\n",
        "clust_1_x2  = new_concat[new_concat[:,-1]==0][:,1]\n",
        "\n",
        "clust_2_x1  = new_concat[new_concat[:,-1]==1][:,0]\n",
        "clust_2_x2  = new_concat[new_concat[:,-1]==1][:,1]"
      ],
      "metadata": {
        "id": "sT7G2FDYxdlq"
      },
      "id": "sT7G2FDYxdlq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cluster Visualisation for the best silhouette score\n",
        "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(10,10))\n",
        "ax[1].scatter(bening_1_pca_1,bening_1_pca_2,label = 'benign',alpha =0.1)\n",
        "ax[1].scatter(malignant_1_pca_1,malignant_1_pca_2,label = 'malignant',alpha=0.1)\n",
        "ax[1].set_xlabel('Principal component 1')\n",
        "ax[1].set_ylabel('Principal component 2')\n",
        "ax[1].legend()\n",
        "ax[0].set_xlabel('Principal component 1')\n",
        "ax[0].set_ylabel('Principal component 2')\n",
        "ax[0].scatter(out_x1,out_x2,alpha =0.5,label = 'outliers')\n",
        "ax[0].scatter(clust_1_x1,clust_1_x2,alpha =0.5,label = 'cluster 1')\n",
        "ax[0].scatter(clust_2_x1,clust_2_x2,alpha =0.5,label = 'cluster 2')\n",
        "ax[0].legend()"
      ],
      "metadata": {
        "id": "gC5boj8YxdiS"
      },
      "id": "gC5boj8YxdiS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cluster Visualisation for the best calinski-harabasz score\n",
        "dbscan = cluster.DBSCAN(eps=eps_ch_score, min_samples=min_samples)\n",
        "clust_labels = dbscan.fit_predict(db_x_red)\n",
        "\n",
        "new_concat = np.concatenate((db_x_red,np.array(clust_labels).reshape((-1,1))),axis=1)\n",
        "\n",
        "out_x1 = new_concat[new_concat[:,-1]==-1][:,0]\n",
        "out_x2 = new_concat[new_concat[:,-1]==-1][:,1]\n",
        "\n",
        "clust_1_x1  = new_concat[new_concat[:,-1]==0][:,0]\n",
        "clust_1_x2  = new_concat[new_concat[:,-1]==0][:,1]\n",
        "\n",
        "clust_2_x1  = new_concat[new_concat[:,-1]==1][:,0]\n",
        "clust_2_x2  = new_concat[new_concat[:,-1]==1][:,1]"
      ],
      "metadata": {
        "id": "voXoLRMHxddC"
      },
      "id": "voXoLRMHxddC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(10,10))\n",
        "ax[1].scatter(bening_1_pca_1,bening_1_pca_2,label = 'benign',alpha =0.1)\n",
        "ax[1].scatter(malignant_1_pca_1,malignant_1_pca_2,label = 'malignant',alpha=0.1)\n",
        "ax[1].set_xlabel('Principal component 1')\n",
        "ax[1].set_ylabel('Principal component 2')\n",
        "ax[1].legend()\n",
        "ax[0].set_xlabel('Principal component 1')\n",
        "ax[0].set_ylabel('Principal component 2')\n",
        "ax[0].scatter(out_x1,out_x2,alpha =0.5,label = 'outliers')\n",
        "ax[0].scatter(clust_1_x1,clust_1_x2,alpha =0.5,label = 'cluster 1')\n",
        "ax[0].scatter(clust_2_x1,clust_2_x2,alpha =0.5,label = 'cluster 2')\n",
        "ax[0].legend()\n"
      ],
      "metadata": {
        "id": "gkzCG2NZ5dDq"
      },
      "id": "gkzCG2NZ5dDq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cluster Visualisation for the best david-bouldin_score\n",
        "dbscan = cluster.DBSCAN(eps=eps_db_score, min_samples=min_samples)\n",
        "clust_labels = dbscan.fit_predict(db_x_red)\n",
        "\n",
        "new_concat = np.concatenate((db_x_red,np.array(clust_labels).reshape((-1,1))),axis=1)\n",
        "\n",
        "out_x1 = new_concat[new_concat[:,-1]==-1][:,0]\n",
        "out_x2 = new_concat[new_concat[:,-1]==-1][:,1]\n",
        "\n",
        "clust_1_x1  = new_concat[new_concat[:,-1]==0][:,0]\n",
        "clust_1_x2  = new_concat[new_concat[:,-1]==0][:,1]\n",
        "\n",
        "clust_2_x1  = new_concat[new_concat[:,-1]==1][:,0]\n",
        "clust_2_x2  = new_concat[new_concat[:,-1]==1][:,1]"
      ],
      "metadata": {
        "id": "rblDzVsK5c9U"
      },
      "id": "rblDzVsK5c9U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(10,10))\n",
        "ax[1].scatter(bening_1_pca_1,bening_1_pca_2,label = 'benign',alpha =0.1)\n",
        "ax[1].scatter(malignant_1_pca_1,malignant_1_pca_2,label = 'malignant',alpha=0.1)\n",
        "ax[1].set_xlabel('Principal component 1')\n",
        "ax[1].set_ylabel('Principal component 2')\n",
        "ax[1].legend()\n",
        "ax[0].set_xlabel('Principal component 1')\n",
        "ax[0].set_ylabel('Principal component 2')\n",
        "ax[0].scatter(out_x1,out_x2,alpha =0.5,label = 'outliers')\n",
        "ax[0].scatter(clust_1_x1,clust_1_x2,alpha =0.5,label = 'cluster 1')\n",
        "ax[0].scatter(clust_2_x1,clust_2_x2,alpha =0.5,label = 'cluster 2')\n",
        "ax[0].legend()"
      ],
      "metadata": {
        "id": "KG4qJTlL5c08"
      },
      "id": "KG4qJTlL5c08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cluster Visualisation for the best Adjusted Rand score\n",
        "dbscan = cluster.DBSCAN(eps=eps_rand_score, min_samples=min_samples)\n",
        "clust_labels = dbscan.fit_predict(db_x_red)\n",
        "\n",
        "new_concat = np.concatenate((db_x_red,np.array(clust_labels).reshape((-1,1))),axis=1)\n",
        "\n",
        "out_x1 = new_concat[new_concat[:,-1]==-1][:,0]\n",
        "out_x2 = new_concat[new_concat[:,-1]==-1][:,1]\n",
        "\n",
        "clust_1_x1  = new_concat[new_concat[:,-1]==0][:,0]\n",
        "clust_1_x2  = new_concat[new_concat[:,-1]==0][:,1]\n",
        "\n",
        "clust_2_x1  = new_concat[new_concat[:,-1]==1][:,0]\n",
        "clust_2_x2  = new_concat[new_concat[:,-1]==1][:,1]"
      ],
      "metadata": {
        "id": "-T3Daaoh-o4e"
      },
      "id": "-T3Daaoh-o4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cluster Visualisation for the best Adjusted Rand score\n",
        "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(10,10))\n",
        "ax[1].scatter(bening_1_pca_1,bening_1_pca_2,label = 'benign',alpha =0.1)\n",
        "ax[1].scatter(malignant_1_pca_1,malignant_1_pca_2,label = 'malignant',alpha=0.1)\n",
        "ax[1].set_xlabel('Principal component 1')\n",
        "ax[1].set_ylabel('Principal component 2')\n",
        "ax[1].legend()\n",
        "ax[0].set_xlabel('Principal component 1')\n",
        "ax[0].set_ylabel('Principal component 2')\n",
        "ax[0].scatter(out_x1,out_x2,alpha =0.5,label = 'outliers')\n",
        "ax[0].scatter(clust_1_x1,clust_1_x2,alpha =0.5,label = 'cluster 1')\n",
        "ax[0].scatter(clust_2_x1,clust_2_x2,alpha =0.5,label = 'cluster 2')\n",
        "ax[0].legend()"
      ],
      "metadata": {
        "id": "GsyZain9-qPH"
      },
      "id": "GsyZain9-qPH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DBSCAN on original dataset with no dimensionality reduction"
      ],
      "metadata": {
        "id": "T8q6l1SK-qKF"
      },
      "id": "T8q6l1SK-qKF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_pca_min_samples = 136\n",
        "\n",
        "non_pca_knearest = NearestNeighbors(n_neighbors=min_samples).fit(X_db)\n",
        "cal_dist,ind = non_pca_knearest.kneighbors(X_db)\n",
        "sort_dist = np.sort(cal_dist,axis=0)[:,1]\n",
        "\n",
        "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8))\n",
        "ax.plot(sort_dist)\n",
        "ax.set_xlabel('Data Point Index')\n",
        "ax.set_ylabel('Average Distance')\n",
        "ax.set_title('Curve to identify optimum eps value for DBSCAN')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TArI_rmn-qG6"
      },
      "id": "TArI_rmn-qG6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The biggest curvature appears to fall within the range of 3.5 to 10. Parameter tuning will now be conducted in this range for the selected min_sample value of 136.\n",
        "\n",
        "For the Original dataset without PCA, the DBSCAN algorithm constantly created a single cluster or no cluster at all. Hence, this approach was abandoned."
      ],
      "metadata": {
        "id": "Lz-m3riM-yPV"
      },
      "id": "Lz-m3riM-yPV"
    },
    {
      "cell_type": "code",
      "source": [
        "non_pca_parameter_score_dict = {'eps':[],'min_samples':[],'cluster_count':[],'homogeneity_score': [],\n",
        "'completeness_score':[],'v_measure_score': [],'adjusted_rand_score': [],\t\n",
        "'silhouette_score': [],'calinski-harabasz_score': [],\t'david-bouldin_score': []}\n",
        "\n",
        "for i in np.arange(3.5,10,0.1):\n",
        "  dbscan = cluster.DBSCAN(eps=i, min_samples=non_pca_min_samples)\n",
        "  clust_labels = dbscan.fit_predict(X_db)\n",
        "  noise_count = list(clust_labels).count(-1)\n",
        "  \n",
        "  if noise_count > 0:\n",
        "    clust_count =  len(set(clust_labels)) - 1\n",
        "  else:\n",
        "    clust_count =  len(set(clust_labels))\n",
        "  non_pca_parameter_score_dict['cluster_count'].append(clust_count)\n",
        "\n",
        "\n",
        "non_pca_parameter_score_dict['cluster_count']"
      ],
      "metadata": {
        "id": "8ApCTJST-qDh"
      },
      "id": "8ApCTJST-qDh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison of Clustering Methods"
      ],
      "metadata": {
        "id": "Ts6iDtOi_Spj"
      },
      "id": "Ts6iDtOi_Spj"
    },
    {
      "cell_type": "code",
      "source": [
        "def frame_val_extract(frame,score_name):\n",
        "  return frame[frame[score_name]==max(frame[score_name])]\n",
        "\n",
        "spectral_clustering_sil_best = frame_val_extract(spectral_clustering_hyperparameter_tuning_n_neighbours_df,'Adjusted_rand_score')\n",
        "spectral_clustering_pca_sil_best = frame_val_extract(pca_spectral_clustering_hyperparameter_tuning_n_neighbours_df,'Adjusted_rand_score')\n",
        "agglomerative_sil_best = frame_val_extract(agglomerative_cluster_hyperparameter_tuning_df,'Adjusted_rand_score')\n",
        "agglomerative_pca_sil_best = frame_val_extract(pca_agglomerative_cluster_hyperparameter_tuning_df,'Adjusted_rand_score')\n",
        "gmm_sil_best = frame_val_extract(gmm_clustering_hyperparameter_tuning_df,'Adjusted_rand_score')\n",
        "gmm_pca_best_sil  = frame_val_extract(gmm_clustering_hyperparameter_tuning_pca_df,'Adjusted_rand_score')\n",
        "kmean_best_sil  = frame_val_extract(kmeans_cluster_hyperparameter_tuning_df,'Adjusted_rand_score')\n",
        "kmeans_pca_best_sil  = frame_val_extract(kmeans_pca_cluster_hyperparameter_tuning_df,'Adjusted_rand_score')\n",
        "dbscan_results_best_sil  = frame_val_extract(dbscan_results,'adjusted_rand_score')"
      ],
      "metadata": {
        "id": "hv73k_Kh-4e8"
      },
      "id": "hv73k_Kh-4e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_names =['Homogeneity Score','Completeness Score','V_measure Score','Adjusted Rand Score','Silhouette Score','Calinski-Harabasz Score','David-Bouldin Score']\n",
        "column_names = ['Spectral Clustering','PCA Spectral Clustering','Agglomerative Clustering','PCA Agglomerative Clustering',\n",
        "                'Gaussian Mixture','PCA Gaussian Mixture','Kmeans','PCA Kmeans','PCA DBSCAN']\n",
        "#Best Silhouette scores for spectral clustering\n",
        "spec_arr =[spectral_clustering_sil_best['Homogeneity Score'].iloc[0]\n",
        ",spectral_clustering_sil_best['Completeness Score'].iloc[0]\n",
        ",spectral_clustering_sil_best['V_measure Score'].iloc[0]\n",
        ",spectral_clustering_sil_best['Adjusted_rand_score'].iloc[0]\n",
        ",spectral_clustering_sil_best['Silhouette Score'].iloc[0]\n",
        ",spectral_clustering_sil_best['Calinski-Harabasz Score'].iloc[0]\n",
        ",spectral_clustering_sil_best['David-Bouldin Score'].iloc[0]]\n",
        "\n",
        "#Best Silhouette scores for pca spectral clustering\n",
        "pca_spec_arr = [spectral_clustering_pca_sil_best['Homogeneity Score'].iloc[0]\n",
        ",spectral_clustering_pca_sil_best['Completeness Score'].iloc[0]\n",
        ",spectral_clustering_pca_sil_best['V_measure Score'].iloc[0]\n",
        ",spectral_clustering_pca_sil_best['Adjusted_rand_score'].iloc[0]\n",
        ",spectral_clustering_pca_sil_best['Silhouette Score'].iloc[0]\n",
        ",spectral_clustering_pca_sil_best['Calinski-Harabasz Score'].iloc[0]\n",
        ",spectral_clustering_pca_sil_best['David-Bouldin Score'].iloc[0]]\n",
        "\n",
        "#Best Silhouette scores for Agglomerative clustering\n",
        "agglomerative_arr = [agglomerative_sil_best['Homogeneity Score'].iloc[0]\n",
        ",agglomerative_sil_best['Completeness Score'].iloc[0]\n",
        ",agglomerative_sil_best['V_measure Score'].iloc[0]\n",
        ",agglomerative_sil_best['Adjusted_rand_score'].iloc[0]\n",
        ",agglomerative_sil_best['Silhouette Score'].iloc[0]\n",
        ",agglomerative_sil_best['Calinski-Harabasz Score'].iloc[0]\n",
        ",agglomerative_sil_best['David-Bouldin Score'].iloc[0]]\n",
        "\n",
        "#Best Silhouette scores for PCA Agglomerative clustering\n",
        "pca_agglomerative_arr =[pca_agglomerative_cluster_hyperparameter_tuning_df['Homogeneity Score'].iloc[0]\n",
        ",pca_agglomerative_cluster_hyperparameter_tuning_df['Completeness Score'].iloc[0]\n",
        ",pca_agglomerative_cluster_hyperparameter_tuning_df['V_measure Score'].iloc[0]\n",
        ",pca_agglomerative_cluster_hyperparameter_tuning_df['Adjusted_rand_score'].iloc[0]\n",
        ",pca_agglomerative_cluster_hyperparameter_tuning_df['Silhouette Score'].iloc[0]\n",
        ",pca_agglomerative_cluster_hyperparameter_tuning_df['Calinski-Harabasz Score'].iloc[0]\n",
        ",pca_agglomerative_cluster_hyperparameter_tuning_df['David-Bouldin Score'].iloc[0]]\n",
        "\n",
        "#Best Silhouette scores for Gaussian Mixture Model\n",
        "gmm_arr =[gmm_sil_best['Homogeneity Score'].iloc[0]\n",
        ",gmm_sil_best['Completeness Score'].iloc[0]\n",
        ",gmm_sil_best['V_measure Score'].iloc[0]\n",
        ",gmm_sil_best['Adjusted_rand_score'].iloc[0]\n",
        ",gmm_sil_best['Silhouette Score'].iloc[0]\n",
        ",gmm_sil_best['Calinski-Harabasz Score'].iloc[0]\n",
        ",gmm_sil_best['David-Bouldin Score'].iloc[0]]\n",
        "\n",
        "#Best Silhouette scores for pca Gaussian Mixture Model\n",
        "pca_gmm_arr =[gmm_pca_best_sil['Homogeneity Score'].iloc[0]\n",
        ",gmm_pca_best_sil['Completeness Score'].iloc[0]\n",
        ",gmm_pca_best_sil['V_measure Score'].iloc[0]\n",
        ",gmm_sil_best['Adjusted_rand_score'].iloc[0]\n",
        ",gmm_pca_best_sil['Silhouette Score'].iloc[0]\n",
        ",gmm_pca_best_sil['Calinski-Harabasz Score'].iloc[0]\n",
        ",gmm_pca_best_sil['David-Bouldin Score'].iloc[0]]\n",
        "\n",
        "#Best Silhouette scores for Kmeans\n",
        "kmeans_arr =[kmean_best_sil['Homogeneity Score'].iloc[0]\n",
        ",kmean_best_sil['Completeness Score'].iloc[0]\n",
        ",kmean_best_sil['V_measure Score'].iloc[0]\n",
        ",kmean_best_sil['Adjusted_rand_score'].iloc[0]\n",
        ",kmean_best_sil['Silhouette Score'].iloc[0]\n",
        ",kmean_best_sil['Calinski-Harabasz Score'].iloc[0]\n",
        ",kmean_best_sil['David-Bouldin Score'].iloc[0]]\n",
        "\n",
        "#Best Silhouette scores for PCA Kmeans\n",
        "kmeans_pca_arr =[kmeans_pca_best_sil['Homogeneity Score'].iloc[0]\n",
        ",kmeans_pca_best_sil['Completeness Score'].iloc[0]\n",
        ",kmeans_pca_best_sil['V_measure Score'].iloc[0]\n",
        ",kmeans_pca_best_sil['Adjusted_rand_score'].iloc[0]\n",
        ",kmeans_pca_best_sil['Silhouette Score'].iloc[0]\n",
        ",kmeans_pca_best_sil['Calinski-Harabasz Score'].iloc[0]\n",
        ",kmeans_pca_best_sil['David-Bouldin Score'].iloc[0]]\n",
        "\n",
        "\n",
        "#Best Silhouette scores for PCA DBSCAN\n",
        "dbscan_pca =[dbscan_results_best_sil['homogeneity_score'].iloc[0]\n",
        ",dbscan_results_best_sil['completeness_score'].iloc[0]\n",
        ",dbscan_results_best_sil['v_measure_score'].iloc[0]\n",
        ",dbscan_results_best_sil['adjusted_rand_score'].iloc[0]\n",
        ",dbscan_results_best_sil['silhouette_score'].iloc[0]\n",
        ",dbscan_results_best_sil['calinski-harabasz_score'].iloc[0]\n",
        ",dbscan_results_best_sil['david-bouldin_score'].iloc[0]]\n",
        "\n",
        "pd.DataFrame({'Spectral Clustering':spec_arr,'PCA Spectral Clustering':pca_spec_arr,'Agglomerative Clustering':agglomerative_arr\n",
        "              ,'PCA Agglomerative Clustering':pca_agglomerative_arr,'Gaussian Mixture':gmm_arr,'PCA Gaussian Mixture':pca_gmm_arr,\n",
        "              'kmeans':kmeans_arr,'PCA Kmeans':kmeans_pca_arr,'PCA DBSCAN':dbscan_pca},index=score_names)\n"
      ],
      "metadata": {
        "id": "XR61devB-4bK"
      },
      "id": "XR61devB-4bK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Coursework_1_Group_24_Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}